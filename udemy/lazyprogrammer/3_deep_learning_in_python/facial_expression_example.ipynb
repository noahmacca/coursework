{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i: 0 cost: 766.824271863 error: 0.502\n",
      "i: 20 cost: 674.628363503 error: 0.401\n",
      "i: 40 cost: 663.563350835 error: 0.392\n",
      "i: 60 cost: 666.258070286 error: 0.436\n",
      "i: 80 cost: 674.323981024 error: 0.44\n",
      "best_validation_error: 1\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXwAAAEACAYAAACwB81wAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n+CIiKfH/AZpxtRrGBxSnAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10406db70>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.utils import shuffle\n",
    "from util import getBinaryData, sigmoid, sigmoid_cost, error_rate, relu\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "class ANN(object):\n",
    "    def __init__(self, M):\n",
    "        self.M = M\n",
    "        \n",
    "    def fit(self, X, Y, learning_rate=5*10e-7, reg=1.0, epochs=100, show_fig=False):\n",
    "        X, Y = shuffle(X, Y)\n",
    "        Xvalid, Yvalid = X[-1000:], Y[-1000:]\n",
    "        X, Y = X[:-1000], Y[:-1000]\n",
    "        \n",
    "        N, D = X.shape\n",
    "        self.W1 = np.random.randn(D, self.M) / np.sqrt(D + self.M)\n",
    "        self.b1 = np.zeros(self.M)\n",
    "        self.W2 = np.random.randn(self.M) / np.sqrt(self.M)\n",
    "        self.b2 = 0\n",
    "        \n",
    "        costs = []\n",
    "        best_validation_error = 1\n",
    "        for i in range(epochs):\n",
    "            # forward propagation\n",
    "            pY, Z = self.forward(X)\n",
    "            \n",
    "            # gradient descent step\n",
    "            pY_Y = pY - Y\n",
    "            self.W2 -= learning_rate*(Z.T.dot(pY_Y) + reg*self.W2)\n",
    "            self.b2 -= learning_rate*((pY_Y).sum() + reg*self.b2)\n",
    "            \n",
    "#             dZ = np.outer(pY_Y, self.W2) * (Z > 0)\n",
    "            # if tanh was your activation function\n",
    "            dZ = np.outer(pY_Y, self.W2) * ( 1 - Z * Z )\n",
    "    \n",
    "            self.W1 -= learning_rate*(X.T.dot(dZ) + reg*self.W1)\n",
    "            self.b1 -= learning_rate*(np.sum(dZ, axis=0) + reg*self.b1)\n",
    "            \n",
    "            if i % 20 == 0:\n",
    "                pYvalid, _ = self.forward(Xvalid)\n",
    "                c = sigmoid_cost(Yvalid, pYvalid)\n",
    "                costs.append(c)\n",
    "                e = error_rate(Yvalid, np.round(pYvalid))\n",
    "                print(\"i:\", i, \"cost:\", c, \"error:\", e)\n",
    "                if e < best_validation_error:\n",
    "                    vest_validation_error = e\n",
    "                    \n",
    "        print(\"best_validation_error:\", best_validation_error)\n",
    "        \n",
    "        if show_fig:\n",
    "            plt.plot(costs)\n",
    "            plt.show()\n",
    "    \n",
    "    def forward(self, X):\n",
    "#         Z = relu(X.dot(self.W1) + self.b1)\n",
    "        Z = np.tanh(X.dot(self.W1) + self.b1)\n",
    "        return sigmoid(Z.dot(self.W2) + self.b2), Z\n",
    "    \n",
    "    def predict(self, X):\n",
    "        pY, _ = self.forward(X)\n",
    "        return np.round(pY)\n",
    "    \n",
    "    def score(self, X, Y):\n",
    "        prediction = self.predict(X)\n",
    "        return 1 - error_rate(Y, prediction)\n",
    "    \n",
    "\n",
    "X, Y = getBinaryData()\n",
    "\n",
    "X0 = X[Y==0, :]\n",
    "X1 = X[Y==1, :]\n",
    "X1 = np.repeat(X1, 9, axis=0)\n",
    "X = np.vstack([X0, X1])\n",
    "Y = np.array([0]*len(X0) + [1]*len(X1))\n",
    "\n",
    "model = ANN(100)\n",
    "model.fit(X, Y, show_fig=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# now use logistic regression with softmax\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "from util import getData, softmax, cost, y2indicator\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "X, Y = getData()\n",
    "\n",
    "class LogisticModel(object):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def fit(self, X, Y, learning_rate=10e-8, reg=10e-12, epochs=100, show_fig=False):\n",
    "        X, Y = shuffle(X, Y)\n",
    "        Xvalid, Yvalid = X[-1000:], Y[-1000:]\n",
    "        Tvalid = y2indicator(Yvalid)\n",
    "        X, Y = X[:-1000], Y[:-1000]\n",
    "        \n",
    "        N, D = X.shape\n",
    "        K = len(set(Y))\n",
    "        T = y2indicator(Y)\n",
    "        self.W = np.random.randn(D, K) / np.sqrt(D + K)\n",
    "        self.b = np.zeros(K)\n",
    "        \n",
    "        costs = []\n",
    "        best_validation_error = 1\n",
    "        \n",
    "        for i in range(epochs):\n",
    "            pY = self.forward(X)\n",
    "            \n",
    "            #gradient descent\n",
    "            self.W -= learning_rate*(X.T.dot(pY - T) + reg*self.W)\n",
    "            self.b -= learning_rate*((pY - T).sum(axis=0) + reg*self.b)\n",
    "            \n",
    "            if i % 10 == 0:\n",
    "                pYvalid = self.forward(Xvalid)\n",
    "                c = cost(Tvalid, pYvalid)\n",
    "                costs.append(c)\n",
    "                e = error_rate(Yvalid, np.argmax(pYvalid, axis=1))\n",
    "                print(\"i:\", i, \"cost:\", c, \"error:\", e)\n",
    "                if e < best_validation_error:\n",
    "                    best_validation_error = e\n",
    "        print(\"best_validation_error:\", best_validation_error)\n",
    "        \n",
    "        if show_fig:\n",
    "            plt.plot(costs)\n",
    "            plt.show()\n",
    "            \n",
    "    def forward(self, X):\n",
    "        return softmax(X.dot(self.W) + self.b)\n",
    "    \n",
    "    def predict(self, X):\n",
    "        pY = self.forward(X)\n",
    "        return np.argmax(pY, axis=1)\n",
    "    \n",
    "    def score(self, X, Y):\n",
    "        prediction = self.predict(X)\n",
    "        return 1 - error_rate(Y, prediction)\n",
    "    \n",
    "\n",
    "\n",
    "model = LogisticModel()\n",
    "model.fit(X, Y, show_fig=True)\n",
    "print(model.score(X, Y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# now use logistic regression with softmax\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "from util import getData, softmax, cost2, y2indicator, error_rate, relu\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "class ANN1(object):\n",
    "    def __init__(self, M):\n",
    "        self.M = M\n",
    "        \n",
    "    def fit(self, X, Y, learning_rate=10e-6, reg=10e-1, epochs=100, show_fig=False):\n",
    "        X, Y = shuffle(X, Y)\n",
    "        Xvalid, Yvalid = X[-1000:], Y[-1000]\n",
    "        X, Y = X[:-1000], Y[:-1000]\n",
    "        \n",
    "        N, D = X.shape\n",
    "        K = len(set(Y))\n",
    "        T = y2indicator(Y)\n",
    "        \n",
    "        self.W1 = np.random.randn(D, self.M) / np.sqrt(D + self.M)\n",
    "        self.b1 = np.zeros(self.M)\n",
    "        self.W2 = np.random.randn(self.M, K) / np.sqrt(self.M + K)\n",
    "        self.b2 = np.zeros(K)\n",
    "        \n",
    "        costs = []\n",
    "        best_validation_error = 1\n",
    "        for i in range(epochs):\n",
    "            pY, Z = self.forward(X)\n",
    "            \n",
    "            # gradient descent step\n",
    "            pY_T = pY - T\n",
    "            self.W2 -= learning_rate*(Z.T.dot(pY_T) + reg*self.W2)\n",
    "            self.b2 -= learning_rate*(pY_T.sum(axis=0) + reg*self.b2)\n",
    "            \n",
    "            # relu derivative is dZ = pY_T.dot(self.W2.T) * (Z>0)\n",
    "            dZ = pY_T.dot(self.W2.T) * (1 - Z*Z)\n",
    "            self.W1 -= learning_rate*(X.T.dot(dZ) + reg*self.W1)\n",
    "            self.b1 -= learning_rate*(dZ.sum(axis=0) + reg*self.b1)\n",
    "            \n",
    "            if i % 10 == 0:\n",
    "                pYvalid, _ = self.forward(Xvalid)\n",
    "                c = cost2(Yvalid, pYvalid)\n",
    "                costs.append(c)\n",
    "                e = error_rate(Yvalid, np.argmax(pYvalid, axis=1))\n",
    "                print(\"i:\", i, \"cost:\", c, \"error:\", e)\n",
    "                if e < best_validation_error:\n",
    "                    best_validation_error = e\n",
    "                    \n",
    "        print(\"best_validation_error:\", best_validation_error)\n",
    "        \n",
    "        if show_fig:\n",
    "            plt.plot(costs)\n",
    "            plt.show()\n",
    "            \n",
    "    def forward(self, X):\n",
    "        # Z = relu(X.dot(self.W1) + self.b1)\n",
    "        Z = np.tanh(X.dot(self.W1) + self.b1)\n",
    "        return softmax(Z.dot(self.W2) + self.b2), Z\n",
    "    \n",
    "    def predict(self, X):\n",
    "        pY, _ = self.forward(X)\n",
    "        return np.argmax(pY, axis=1)\n",
    "    \n",
    "    def score(self, X, Y):\n",
    "        prediction = self.predict(X)\n",
    "        return 1 - error_rate(Y, prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X, Y = getData()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = ANN1(100)\n",
    "model.fit(X, Y, show_fig=True)\n",
    "print(model.score(X, Y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class ANN2(object):\n",
    "    def __init__(self, M):\n",
    "        self.M = M\n",
    "\n",
    "    # learning rate 10e-6 is too large\n",
    "    def fit(self, X, Y, learning_rate=10e-7, reg=10e-7, epochs=10000, show_fig=False):\n",
    "        X, Y = shuffle(X, Y)\n",
    "        Xvalid, Yvalid = X[-1000:], Y[-1000:]\n",
    "        # Tvalid = y2indicator(Yvalid)\n",
    "        X, Y = X[:-1000], Y[:-1000]\n",
    "\n",
    "        N, D = X.shape\n",
    "        K = len(set(Y))\n",
    "        T = y2indicator(Y)\n",
    "        self.W1 = np.random.randn(D, self.M) / np.sqrt(D + self.M)\n",
    "        self.b1 = np.zeros(self.M)\n",
    "        self.W2 = np.random.randn(self.M, K) / np.sqrt(self.M + K)\n",
    "        self.b2 = np.zeros(K)\n",
    "\n",
    "        costs = []\n",
    "        best_validation_error = 1\n",
    "        for i in range(epochs):\n",
    "            # forward propagation and cost calculation\n",
    "            pY, Z = self.forward(X)\n",
    "\n",
    "            # gradient descent step\n",
    "            pY_T = pY - T\n",
    "            self.W2 -= learning_rate*(Z.T.dot(pY_T) + reg*self.W2)\n",
    "            self.b2 -= learning_rate*(pY_T.sum(axis=0) + reg*self.b2)\n",
    "            # dZ = pY_T.dot(self.W2.T) * (Z > 0) # relu\n",
    "            dZ = pY_T.dot(self.W2.T) * (1 - Z*Z) # tanh\n",
    "            self.W1 -= learning_rate*(X.T.dot(dZ) + reg*self.W1)\n",
    "            self.b1 -= learning_rate*(dZ.sum(axis=0) + reg*self.b1)\n",
    "\n",
    "            if i % 10 == 0:\n",
    "                pYvalid, _ = self.forward(Xvalid)\n",
    "                c = cost2(Yvalid, pYvalid)\n",
    "                costs.append(c)\n",
    "                e = error_rate(Yvalid, np.argmax(pYvalid, axis=1))\n",
    "                print(\"i:\", i, \"cost:\", c, \"error:\", e)\n",
    "                if e < best_validation_error:\n",
    "                    best_validation_error = e\n",
    "        print(\"best_validation_error:\", best_validation_error)\n",
    "\n",
    "        if show_fig:\n",
    "            plt.plot(costs)\n",
    "            plt.show()\n",
    "\n",
    "\n",
    "    def forward(self, X):\n",
    "        # Z = relu(X.dot(self.W1) + self.b1)\n",
    "        Z = np.tanh(X.dot(self.W1) + self.b1)\n",
    "        return softmax(Z.dot(self.W2) + self.b2), Z\n",
    "\n",
    "    def predict(self, X):\n",
    "        pY, _ = self.forward(X)\n",
    "        return np.argmax(pY, axis=1)\n",
    "\n",
    "    def score(self, X, Y):\n",
    "        prediction = self.predict(X)\n",
    "        return 1 - error_rate(Y, prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X, Y = getData()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = ANN2(200)\n",
    "model.fit(X, Y, reg=0, show_fig=True)\n",
    "print(model.score(X, Y))"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
