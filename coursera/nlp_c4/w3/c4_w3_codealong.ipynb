{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenization\n",
    "\n",
    "* Vector of integers. Have to decide if you will break down to words, morphological units, phonemes, or characters\n",
    "* This depends on language as  well\n",
    "* Memory limitations on vocab size\n",
    "\n",
    "* BERT and variants use a modified version of byte-pair encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "é = é : False\n"
     ]
    }
   ],
   "source": [
    "eaccent = '\\u00E9'\n",
    "e_accent = '\\u0065\\u0301'\n",
    "print(f'{eaccent} = {e_accent} : {eaccent == e_accent}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "é = é : True\n"
     ]
    }
   ],
   "source": [
    "# NFKC normalizes these\n",
    "from unicodedata import normalize\n",
    "\n",
    "norm_eaccent = normalize('NFKC', '\\u00E9')\n",
    "norm_e_accent = normalize('NFKC', '\\u0065\\u0301')\n",
    "print(f'{norm_eaccent} = {norm_e_accent} : {norm_eaccent == norm_e_accent}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "é : 0xe9\n",
      "é : 0x65 0x301\n",
      "é : 0xe9\n",
      "é : 0xe9\n"
     ]
    }
   ],
   "source": [
    "def get_hex_encoding(s):\n",
    "    return ' '.join(hex(ord(c)) for c in s)\n",
    "\n",
    "def print_string_and_encoding(s):\n",
    "    print(f'{s} : {get_hex_encoding(s)}')\n",
    "    \n",
    "for s in [eaccent, e_accent, norm_eaccent, norm_e_accent]:\n",
    "    print_string_and_encoding(s)\n",
    "    \n",
    "# Unicode code changed in normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0x54 0x6f 0x6b 0x65 0x6e 0x69 0x7a 0x61 0x74 0x69 0x6f 0x6e 0x20 0x69 0x73 0x20 0x68 0x61 0x72 0x64 0x2e\n",
      "0x54 0x6f 0x6b 0x65 0x6e 0x69 0x7a 0x61 0x74 0x69 0x6f 0x6e 0x2581 0x69 0x73 0x2581 0x68 0x61 0x72 0x64 0x2e\n",
      "0x54 0x6f 0x6b 0x65 0x6e 0x69 0x7a 0x61 0x74 0x69 0x6f 0x6e 0x20 0x69 0x73 0x20 0x68 0x61 0x72 0x64 0x2e\n"
     ]
    }
   ],
   "source": [
    "s = 'Tokenization is hard.'\n",
    "s_ = s.replace(' ',  '\\u2581')\n",
    "s_n = normalize('NFKC', 'Tokenization is hard.')\n",
    "\n",
    "print(get_hex_encoding(s))\n",
    "print(get_hex_encoding(s_))\n",
    "print(get_hex_encoding(s_n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0x54 0x6f 0x6b 0x65 0x6e 0x69 0x7a 0x61 0x74 0x69 0x6f 0x6e 0x20 0x69 0x73 0x20 0x68 0x61 0x72 0x64 0x2e\n",
      "0x54 0x6f 0x6b 0x65 0x6e 0x69 0x7a 0x61 0x74 0x69 0x6f 0x6e 0x20 0x69 0x73 0x20 0x68 0x61 0x72 0x64 0x2e\n",
      "0x54 0x6f 0x6b 0x65 0x6e 0x69 0x7a 0x61 0x74 0x69 0x6f 0x6e 0x2581 0x69 0x73 0x2581 0x68 0x61 0x72 0x64 0x2e\n"
     ]
    }
   ],
   "source": [
    "s = 'Tokenization is hard.'\n",
    "sn = normalize('NFKC', 'Tokenization is hard.')\n",
    "sn_ = s.replace(' ', '\\u2581')\n",
    "\n",
    "print(get_hex_encoding(s))\n",
    "print(get_hex_encoding(sn))\n",
    "print(get_hex_encoding(sn_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beginners BBQ Class Taking Place in Missoula!\n",
      "Do you want to get better at making delicious BBQ? You will have the opportunity, put this on your calendar now. Thursday, September 22nd join World Class BBQ Champion, Tony Balay from Lonestar Smoke Rangers. He will be teaching a beginner level class for everyone who wants to get better with their culinary skills.\n",
      "He will teach you everything you need to know to compete in a KCBS BBQ competition, including techniques, recipes, timelines, meat selection and trimming, plus smoker and fire information.\n",
      "The cost to be in the class is $35 per person, and for spectators it is free. Included in the cost will be either a t-shirt or apron and you will be tasting samples of each meat that is prepared.\n",
      "\n",
      "Discussion in 'Mac OS X Lion (10.7)' started by axboi87, Jan 20, 2012.\n",
      "I've got a 500gb internal drive and a 240gb SSD.\n",
      "When trying to restore using di\n"
     ]
    }
   ],
   "source": [
    "# BPE Algorithm\n",
    "import ast\n",
    "\n",
    "def convert_json_examples_to_text(filepath):\n",
    "    example_jsons = list(map(ast.literal_eval, open(filepath))) # Read in the json from the example file\n",
    "    texts = [example_json['text'].decode('utf-8') for example_json in example_jsons] # Decode the byte sequences\n",
    "    text = '\\n\\n'.join(texts)       # Separate different articles by two newlines\n",
    "    text = normalize('NFKC', text)  # Normalize the text\n",
    "\n",
    "    with open('example.txt', 'w') as fw:\n",
    "        fw.write(text)\n",
    "    \n",
    "    return text\n",
    "\n",
    "text = convert_json_examples_to_text('data.txt')\n",
    "print(text[:900])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "vocab = Counter(['\\u2581' + word for word in text.split()])\n",
    "vocab = {' '.join([l for l in word]): freq for word, freq in vocab.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "▁ B e g i n n e r s: 1\n",
      "▁ B B Q: 3\n",
      "▁ C l a s s: 2\n",
      "▁ T a k i n g: 1\n",
      "▁ P l a c e: 1\n",
      "▁ i n: 15\n",
      "▁ M i s s o u l a !: 1\n",
      "▁ D o: 1\n",
      "▁ y o u: 13\n",
      "▁ w a n t: 1\n",
      "▁ t o: 33\n",
      "▁ g e t: 2\n",
      "▁ b e t t e r: 2\n",
      "▁ a t: 1\n",
      "▁ m a k i n g: 2\n",
      "▁ d e l i c i o u s: 1\n",
      "▁ B B Q ?: 1\n",
      "▁ Y o u: 1\n",
      "▁ w i l l: 6\n",
      "▁ h a v e: 4\n",
      "▁ t h e: 31\n"
     ]
    }
   ],
   "source": [
    "def show_vocab(vocab, end='\\n', limit=20):\n",
    "    shown = 0\n",
    "    for word, freq in vocab.items():\n",
    "        print(f'{word}: {freq}', end=end)\n",
    "        shown +=1\n",
    "        if shown > limit:\n",
    "            break\n",
    "            \n",
    "            \n",
    "show_vocab(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of unique words: 455\n",
      "Number of merges required to reproduce SentencePiece training on the whole corpus: 273\n"
     ]
    }
   ],
   "source": [
    "print(f'Total number of unique words: {len(vocab)}')\n",
    "print(f'Number of merges required to reproduce SentencePiece training on the whole corpus: {int(0.60*len(vocab))}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BPE\n",
    "- AKA Byte Pair Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re, collections\n",
    "\n",
    "def get_stats(vocab):\n",
    "    pairs = collections.defaultdict(int)\n",
    "    for word, freq in vocab.items():\n",
    "        symbols = word.split()\n",
    "        for i in range(len(symbols) - 1):\n",
    "            pairs[symbols[i], symbols[i+1]] += freq\n",
    "    return pairs\n",
    "\n",
    "def merge_vocab(pair, v_in):\n",
    "    v_out = {}\n",
    "    bigram = re.escape(' '.join(pair))\n",
    "    p = re.compile(r'(?<!\\S)' + bigram + r'(?!\\S)')\n",
    "    for word in v_in:\n",
    "        w_out = p.sub(''.join(pair), word)\n",
    "        v_out[w_out] = v_in[word]\n",
    "    return v_out\n",
    "\n",
    "def get_sentence_piece_vocab(vocab, frac_merges=0.60):\n",
    "    sp_vocab = vocab.copy()\n",
    "    num_merges = int(len(sp_vocab)*frac_merges)\n",
    "    \n",
    "    for i in range(num_merges):\n",
    "        pairs = get_stats(sp_vocab)\n",
    "        best = max(pairs, key=pairs.get)\n",
    "        sp_vocab = merge_vocab(best, sp_vocab)\n",
    "\n",
    "    return sp_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "▁B e g in n ers: 1\n",
      "▁BBQ: 3\n",
      "▁Cl ass: 2\n",
      "▁T ak ing: 1\n",
      "▁P la ce: 1\n",
      "▁in: 15\n",
      "▁M is s ou la !: 1\n",
      "▁D o: 1\n",
      "▁you: 13\n",
      "▁w an t: 1\n",
      "▁to: 33\n",
      "▁g et: 2\n",
      "▁be t ter: 2\n",
      "▁a t: 1\n",
      "▁mak ing: 2\n",
      "▁d e l ic i ou s: 1\n",
      "▁BBQ ?: 1\n",
      "▁ Y ou: 1\n",
      "▁will: 6\n",
      "▁have: 4\n",
      "▁the: 31\n"
     ]
    }
   ],
   "source": [
    "sp_vocab = get_sentence_piece_vocab(vocab)\n",
    "show_vocab(sp_vocab) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on SentencePieceProcessor in module sentencepiece object:\n",
      "\n",
      "class SentencePieceProcessor(builtins.object)\n",
      " |  SentencePieceProcessor(model_file=None, model_proto=None, out_type=<class 'int'>, add_bos=False, add_eos=False, reverse=False, enable_sampling=False, nbest_size=-1, alpha=0.1)\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  Decode(self, input)\n",
      " |      Decode processed id or token sequences.\n",
      " |  \n",
      " |  DecodeIds = DecodeIdsWithCheck(self, ids)\n",
      " |  \n",
      " |  DecodeIdsAsSerializedProto = DecodeIdsAsSerializedProtoWithCheck(self, ids)\n",
      " |  \n",
      " |  DecodeIdsAsSerializedProtoWithCheck(self, ids)\n",
      " |  \n",
      " |  DecodeIdsWithCheck(self, ids)\n",
      " |  \n",
      " |  DecodePieces(self, pieces)\n",
      " |  \n",
      " |  DecodePiecesAsSerializedProto(self, pieces)\n",
      " |  \n",
      " |  Detokenize = Decode(self, input)\n",
      " |  \n",
      " |  Encode(self, input, out_type=None, add_bos=None, add_eos=None, reverse=None, enable_sampling=None, nbest_size=None, alpha=None)\n",
      " |      Encode text input to segmented ids or tokens.\n",
      " |      \n",
      " |      Args:\n",
      " |      input: input string. accepsts list of string.\n",
      " |      out_type: output type. int or str.\n",
      " |      add_bos: Add <s> to the result (Default = false)\n",
      " |      add_eos: Add </s> to the result (Default = false) <s>/</s> is added after\n",
      " |        reversing (if enabled).\n",
      " |      reverse: Reverses the tokenized sequence (Default = false)\n",
      " |      nbest_size: sampling parameters for unigram. Invalid for BPE-Dropout.\n",
      " |                  nbest_size = {0,1}: No sampling is performed.\n",
      " |                  nbest_size > 1: samples from the nbest_size results.\n",
      " |                  nbest_size < 0: assuming that nbest_size is infinite and samples\n",
      " |                    from the all hypothesis (lattice) using\n",
      " |                    forward-filtering-and-backward-sampling algorithm.\n",
      " |      alpha: Soothing parameter for unigram sampling, and merge probability for\n",
      " |             BPE-dropout (probablity 'p' in BPE-dropout paper).\n",
      " |  \n",
      " |  EncodeAsIds(self, input)\n",
      " |  \n",
      " |  EncodeAsPieces(self, input)\n",
      " |  \n",
      " |  EncodeAsSerializedProto(self, input)\n",
      " |  \n",
      " |  GetEncoderVersion(self)\n",
      " |  \n",
      " |  GetPieceSize(self)\n",
      " |  \n",
      " |  GetScore = _batched_func(self, arg)\n",
      " |  \n",
      " |  IdToPiece = _batched_func(self, arg)\n",
      " |  \n",
      " |  Init(self, model_file=None, model_proto=None, out_type=<class 'int'>, add_bos=False, add_eos=False, reverse=False, enable_sampling=False, nbest_size=-1, alpha=0.1)\n",
      " |      Initialzie sentencepieceProcessor.\n",
      " |      \n",
      " |      Args:\n",
      " |        model_file: The sentencepiece model file path.\n",
      " |        model_proto: The sentencepiece model serialized proto.\n",
      " |        out_type: output type. int or str.\n",
      " |        add_bos: Add <s> to the result (Default = false)\n",
      " |        add_eos: Add </s> to the result (Default = false) <s>/</s> is added after\n",
      " |          reversing (if enabled).\n",
      " |        reverse: Reverses the tokenized sequence (Default = false)\n",
      " |        nbest_size: sampling parameters for unigram. Invalid for BPE-Dropout.\n",
      " |                    nbest_size = {0,1}: No sampling is performed.\n",
      " |                    nbest_size > 1: samples from the nbest_size results.\n",
      " |                    nbest_size < 0: assuming that nbest_size is infinite and samples\n",
      " |                      from the all hypothesis (lattice) using\n",
      " |                      forward-filtering-and-backward-sampling algorithm.\n",
      " |        alpha: Soothing parameter for unigram sampling, and dropout probability of\n",
      " |          merge operations for BPE-dropout.\n",
      " |  \n",
      " |  IsByte = _batched_func(self, arg)\n",
      " |  \n",
      " |  IsControl = _batched_func(self, arg)\n",
      " |  \n",
      " |  IsUnknown = _batched_func(self, arg)\n",
      " |  \n",
      " |  IsUnused = _batched_func(self, arg)\n",
      " |  \n",
      " |  Load(self, model_file=None, model_proto=None)\n",
      " |      Overwride SentencePieceProcessor.Load to support both model_file and model_proto.\n",
      " |      \n",
      " |      Args:\n",
      " |        model_file: The sentencepiece model file path.\n",
      " |        model_proto: The sentencepiece model serialized proto. Either `model_file`\n",
      " |          or `model_proto` must be set.\n",
      " |  \n",
      " |  LoadFromFile(self, arg)\n",
      " |  \n",
      " |  LoadFromSerializedProto(self, serialized)\n",
      " |  \n",
      " |  LoadVocabulary(self, filename, threshold)\n",
      " |  \n",
      " |  NBestEncodeAsIds(self, input, nbest_size)\n",
      " |  \n",
      " |  NBestEncodeAsPieces(self, input, nbest_size)\n",
      " |  \n",
      " |  NBestEncodeAsSerializedProto(self, input, nbest_size)\n",
      " |  \n",
      " |  PieceToId = _batched_func(self, arg)\n",
      " |  \n",
      " |  ResetVocabulary(self)\n",
      " |  \n",
      " |  SampleEncodeAsIds(self, input, nbest_size, alpha)\n",
      " |  \n",
      " |  SampleEncodeAsPieces(self, input, nbest_size, alpha)\n",
      " |  \n",
      " |  SampleEncodeAsSerializedProto(self, input, nbest_size, alpha)\n",
      " |  \n",
      " |  SetDecodeExtraOptions(self, extra_option)\n",
      " |  \n",
      " |  SetEncodeExtraOptions(self, extra_option)\n",
      " |  \n",
      " |  SetEncoderVersion(self, encoder_version)\n",
      " |  \n",
      " |  SetVocabulary(self, valid_vocab)\n",
      " |  \n",
      " |  Tokenize = Encode(self, input, out_type=None, add_bos=None, add_eos=None, reverse=None, enable_sampling=None, nbest_size=None, alpha=None)\n",
      " |  \n",
      " |  __getitem__(self, piece)\n",
      " |  \n",
      " |  __getstate__(self)\n",
      " |  \n",
      " |  __init__ = Init(self, model_file=None, model_proto=None, out_type=<class 'int'>, add_bos=False, add_eos=False, reverse=False, enable_sampling=False, nbest_size=-1, alpha=0.1)\n",
      " |  \n",
      " |  __len__(self)\n",
      " |  \n",
      " |  __repr__ = _swig_repr(self)\n",
      " |  \n",
      " |  __setstate__(self, serialized_model_proto)\n",
      " |  \n",
      " |  bos_id(self)\n",
      " |  \n",
      " |  decode = Decode(self, input)\n",
      " |  \n",
      " |  decode_ids = DecodeIdsWithCheck(self, ids)\n",
      " |  \n",
      " |  decode_ids_as_serialized_proto = DecodeIdsAsSerializedProtoWithCheck(self, ids)\n",
      " |  \n",
      " |  decode_ids_as_serialized_proto_with_check = DecodeIdsAsSerializedProtoWithCheck(self, ids)\n",
      " |  \n",
      " |  decode_ids_with_check = DecodeIdsWithCheck(self, ids)\n",
      " |  \n",
      " |  decode_pieces = DecodePieces(self, pieces)\n",
      " |  \n",
      " |  decode_pieces_as_serialized_proto = DecodePiecesAsSerializedProto(self, pieces)\n",
      " |  \n",
      " |  detokenize = Decode(self, input)\n",
      " |  \n",
      " |  encode = Encode(self, input, out_type=None, add_bos=None, add_eos=None, reverse=None, enable_sampling=None, nbest_size=None, alpha=None)\n",
      " |  \n",
      " |  encode_as_ids = EncodeAsIds(self, input)\n",
      " |  \n",
      " |  encode_as_pieces = EncodeAsPieces(self, input)\n",
      " |  \n",
      " |  encode_as_serialized_proto = EncodeAsSerializedProto(self, input)\n",
      " |  \n",
      " |  eos_id(self)\n",
      " |  \n",
      " |  get_encoder_version = GetEncoderVersion(self)\n",
      " |  \n",
      " |  get_piece_size = GetPieceSize(self)\n",
      " |  \n",
      " |  get_score = _batched_func(self, arg)\n",
      " |  \n",
      " |  id_to_piece = _batched_func(self, arg)\n",
      " |  \n",
      " |  init = Init(self, model_file=None, model_proto=None, out_type=<class 'int'>, add_bos=False, add_eos=False, reverse=False, enable_sampling=False, nbest_size=-1, alpha=0.1)\n",
      " |  \n",
      " |  is_byte = _batched_func(self, arg)\n",
      " |  \n",
      " |  is_control = _batched_func(self, arg)\n",
      " |  \n",
      " |  is_unknown = _batched_func(self, arg)\n",
      " |  \n",
      " |  is_unused = _batched_func(self, arg)\n",
      " |  \n",
      " |  load = Load(self, model_file=None, model_proto=None)\n",
      " |  \n",
      " |  load_from_file = LoadFromFile(self, arg)\n",
      " |  \n",
      " |  load_from_serialized_proto = LoadFromSerializedProto(self, serialized)\n",
      " |  \n",
      " |  load_vocabulary = LoadVocabulary(self, filename, threshold)\n",
      " |  \n",
      " |  nbest_encode_as_ids = NBestEncodeAsIds(self, input, nbest_size)\n",
      " |  \n",
      " |  nbest_encode_as_pieces = NBestEncodeAsPieces(self, input, nbest_size)\n",
      " |  \n",
      " |  nbest_encode_as_serialized_proto = NBestEncodeAsSerializedProto(self, input, nbest_size)\n",
      " |  \n",
      " |  pad_id(self)\n",
      " |  \n",
      " |  piece_size(self)\n",
      " |  \n",
      " |  piece_to_id = _batched_func(self, arg)\n",
      " |  \n",
      " |  reset_vocabulary = ResetVocabulary(self)\n",
      " |  \n",
      " |  sample_encode_as_ids = SampleEncodeAsIds(self, input, nbest_size, alpha)\n",
      " |  \n",
      " |  sample_encode_as_pieces = SampleEncodeAsPieces(self, input, nbest_size, alpha)\n",
      " |  \n",
      " |  sample_encode_as_serialized_proto = SampleEncodeAsSerializedProto(self, input, nbest_size, alpha)\n",
      " |  \n",
      " |  serialized_model_proto(self)\n",
      " |  \n",
      " |  set_decode_extra_options = SetDecodeExtraOptions(self, extra_option)\n",
      " |  \n",
      " |  set_encode_extra_options = SetEncodeExtraOptions(self, extra_option)\n",
      " |  \n",
      " |  set_encoder_version = SetEncoderVersion(self, encoder_version)\n",
      " |  \n",
      " |  set_vocabulary = SetVocabulary(self, valid_vocab)\n",
      " |  \n",
      " |  tokenize = Encode(self, input, out_type=None, add_bos=None, add_eos=None, reverse=None, enable_sampling=None, nbest_size=None, alpha=None)\n",
      " |  \n",
      " |  unk_id(self)\n",
      " |  \n",
      " |  vocab_size(self)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Static methods defined here:\n",
      " |  \n",
      " |  __swig_destroy__ = delete_SentencePieceProcessor(...)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors defined here:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  thisown\n",
      " |      The membership flag\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# SentencePiece model\n",
    "import sentencepiece as spm\n",
    "sp = spm.SentencePieceProcessor(model_file='sentencepiece.model')\n",
    "help(sp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['▁Beginn', 'ers', '▁BBQ', '▁Class', '▁', 'Taking', '▁Place', '▁in', '▁Miss', 'oul', 'a', '!']\n",
      "[12847, 277, 15068, 4501, 3, 12297, 3399, 16, 5964, 7115, 9, 55]\n",
      "Beginners BBQ Class Taking Place in Missoula!\n",
      "Beginners\n"
     ]
    }
   ],
   "source": [
    "s0 = 'Beginners BBQ Class Taking Place in Missoula!'\n",
    "# encode: text => id\n",
    "print(sp.encode_as_pieces(s0))\n",
    "print(sp.encode_as_ids(s0))\n",
    "\n",
    "# decode: id => text\n",
    "print(sp.decode_pieces(sp.encode_as_pieces(s0)))\n",
    "print(sp.decode_ids([12847, 277]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SentencePiece for ID 15068: ▁BBQ\n",
      "ID for Sentence Piece ▁BBQ: 15068\n",
      "ID for unknown text __MUST_BE_UNKNOWN__: 2\n"
     ]
    }
   ],
   "source": [
    "uid = 15068\n",
    "spiece = \"\\u2581BBQ\"\n",
    "unknown = \"__MUST_BE_UNKNOWN__\"\n",
    "\n",
    "# id <=> piece conversion\n",
    "print(f'SentencePiece for ID {uid}: {sp.id_to_piece(uid)}')\n",
    "print(f'ID for Sentence Piece {spiece}: {sp.piece_to_id(spiece)}')\n",
    "\n",
    "# returns 0 for unknown tokens (we can change the id for UNK)\n",
    "print(f'ID for unknown text {unknown}: {sp.piece_to_id(unknown)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beginning of sentence id: -1\n",
      "Pad id: 0\n",
      "End of sentence id: 1\n",
      "Unknown id: 2\n",
      "Vocab size: 32000\n"
     ]
    }
   ],
   "source": [
    "print(f'Beginning of sentence id: {sp.bos_id()}')\n",
    "print(f'Pad id: {sp.pad_id()}')\n",
    "print(f'End of sentence id: {sp.eos_id()}')\n",
    "print(f'Unknown id: {sp.unk_id()}')\n",
    "print(f'Vocab size: {sp.vocab_size()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Id\tSentP\tControl?\n",
      "------------------------\n",
      "0\t<pad>\tTrue\n",
      "1\t</s>\tTrue\n",
      "2\t<unk>\tFalse\n",
      "3\t▁\tFalse\n",
      "4\tX\tFalse\n",
      "5\t.\tFalse\n",
      "6\t,\tFalse\n",
      "7\ts\tFalse\n",
      "8\t▁the\tFalse\n",
      "9\ta\tFalse\n"
     ]
    }
   ],
   "source": [
    "print('\\nId\\tSentP\\tControl?')\n",
    "print('------------------------')\n",
    "# <unk>, <s>, </s> are defined by default. Their ids are (0, 1, 2)\n",
    "# <s> and </s> are defined as 'control' symbol.\n",
    "for uid in range(10):\n",
    "    print(uid, sp.id_to_piece(uid), sp.is_control(uid), sep='\\t')\n",
    "    \n",
    "# for uid in range(sp.vocab_size()-10,sp.vocab_size()):\n",
    "#     print(uid, sp.id_to_piece(uid), sp.is_control(uid), sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** BPE ***\n",
      "['▁B', 'e', 'ginn', 'ers', '▁BBQ', '▁Cl', 'ass', '▁T', 'ak', 'ing', '▁P', 'la', 'ce', '▁in', '▁M', 'is', 's', 'ou', 'la', '!']\n"
     ]
    }
   ],
   "source": [
    "spm.SentencePieceTrainer.train('--input=example.txt --model_prefix=example_bpe --vocab_size=450 --model_type=bpe')\n",
    "sp_bpe = spm.SentencePieceProcessor()\n",
    "sp_bpe.load('example_bpe.model')\n",
    "\n",
    "print('*** BPE ***')\n",
    "print(sp_bpe.encode_as_pieces(s0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "▁B e g in n ers: 1, ▁BBQ: 3, ▁Cl ass: 2, ▁T ak ing: 1, ▁P la ce: 1, ▁in: 15, ▁M is s ou la !: 1, ▁D o: 1, ▁you: 13, ▁w an t: 1, ▁to: 33, ▁g et: 2, ▁be t ter: 2, ▁a t: 1, ▁mak ing: 2, ▁d e l ic i ou s: 1, ▁BBQ ?: 1, ▁ Y ou: 1, ▁will: 6, ▁have: 4, ▁the: 31, "
     ]
    }
   ],
   "source": [
    "show_vocab(sp_vocab, end = ', ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 1, 1, 2, 2, 3, 3, 4, 4]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from heapq import heappush, heappop\n",
    "\n",
    "def heapsort(iterable):\n",
    "    h = []\n",
    "    for value in iterable:\n",
    "        heappush(h, value)\n",
    "    return [heappop(h) for i in range(len(h))]\n",
    "\n",
    "a = [1,4,3,1,3,2,1,4,2]\n",
    "heapsort(a)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "coursera",
   "language": "python",
   "name": "coursera"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
