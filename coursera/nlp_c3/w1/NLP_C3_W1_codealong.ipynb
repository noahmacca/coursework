{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from trax import layers as tl\n",
    "from trax import shapes\n",
    "from trax import fastmath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "-- Properties --\nname : Relu\nexpected inputs : 1\npromised outputs : 1 \n\n-- Inputs --\nx : [-2 -1  0  1  2] \n\n-- Outputs --\ny : [0 0 0 1 2]\n"
     ]
    }
   ],
   "source": [
    "relu = tl.Relu()\n",
    "\n",
    "# Inspect properties\n",
    "print(\"-- Properties --\")\n",
    "print(\"name :\", relu.name)\n",
    "print(\"expected inputs :\", relu.n_in)\n",
    "print(\"promised outputs :\", relu.n_out, \"\\n\")\n",
    "\n",
    "# Inputs\n",
    "x = np.array([-2, -1, 0, 1, 2])\n",
    "print(\"-- Inputs --\")\n",
    "print(\"x :\", x, \"\\n\")\n",
    "\n",
    "# Outputs\n",
    "y = relu(x)\n",
    "print(\"-- Outputs --\")\n",
    "print(\"y :\", y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "-- Properties --\nname : Concatenate\nexpected inputs : 2\npromised outputs : 1 \n\n-- Inputs --\nx1 : [-10 -20 -30]\nx2 : [1. 2. 3.] \n\n-- Outputs --\ny : [-10. -20. -30.   1.   2.   3.]\n"
     ]
    }
   ],
   "source": [
    "# Concatenate layer\n",
    "concat = tl.Concatenate()\n",
    "\n",
    "print(\"-- Properties --\")\n",
    "print(\"name :\", concat.name)\n",
    "print(\"expected inputs :\", concat.n_in)\n",
    "print(\"promised outputs :\", concat.n_out, \"\\n\")\n",
    "\n",
    "# Inputs\n",
    "x1 = np.array([-10, -20, -30])\n",
    "x2 = x1 / -10\n",
    "print(\"-- Inputs --\")\n",
    "print(\"x1 :\", x1)\n",
    "print(\"x2 :\", x2, \"\\n\")\n",
    "\n",
    "# Outputs\n",
    "y = concat([x1, x2])\n",
    "print(\"-- Outputs --\")\n",
    "print(\"y :\", y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "-- Properties --\nname : Concatenate\nexpected inputs : 3\npromised outputs : 1 \n\n-- Inputs --\nx1 : [-10 -20 -30]\nx2 : [1. 2. 3.]\nx3 : [0.99 1.98 2.97] \n\n-- Outputs --\ny : [-10.   -20.   -30.     1.     2.     3.     0.99   1.98   2.97]\n"
     ]
    }
   ],
   "source": [
    "# Configure a concatenate layer\n",
    "concat_3 = tl.Concatenate(n_items=3)  # configure the layer's expected inputs\n",
    "print(\"-- Properties --\")\n",
    "print(\"name :\", concat_3.name)\n",
    "print(\"expected inputs :\", concat_3.n_in)\n",
    "print(\"promised outputs :\", concat_3.n_out, \"\\n\")\n",
    "\n",
    "# Inputs\n",
    "x1 = np.array([-10, -20, -30])\n",
    "x2 = x1 / -10\n",
    "x3 = x2 * 0.99\n",
    "print(\"-- Inputs --\")\n",
    "print(\"x1 :\", x1)\n",
    "print(\"x2 :\", x2)\n",
    "print(\"x3 :\", x3, \"\\n\")\n",
    "\n",
    "# Outputs\n",
    "y = concat_3([x1, x2, x3])\n",
    "print(\"-- Outputs --\")\n",
    "print(\"y :\", y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Help on class Concatenate in module trax.layers.combinators:\n",
      "\n",
      "class Concatenate(trax.layers.base.Layer)\n",
      " |  Concatenate(n_items=2, axis=-1)\n",
      " |  \n",
      " |  Concatenates n tensors into a single tensor.\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      Concatenate\n",
      " |      trax.layers.base.Layer\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, n_items=2, axis=-1)\n",
      " |      Creates a partially initialized, unconnected layer instance.\n",
      " |      \n",
      " |      Args:\n",
      " |        n_in: Number of inputs expected by this layer.\n",
      " |        n_out: Number of outputs promised by this layer.\n",
      " |        name: Class-like name for this layer; for use when printing this layer.\n",
      " |        sublayers_to_print: Sublayers to display when printing out this layer;\n",
      " |          By default (when None) we display all sublayers.\n",
      " |  \n",
      " |  forward(self, xs)\n",
      " |      Computes this layer's output as part of a forward pass through the model.\n",
      " |      \n",
      " |      Authors of new layer subclasses should override this method to define the\n",
      " |      forward computation that their layer performs. Use `self.weights` to access\n",
      " |      trainable weights of this layer. If you need to use local non-trainable\n",
      " |      state or randomness, use `self.rng` for the random seed (no need to set it)\n",
      " |      and use `self.state` for non-trainable state (and set it to the new value).\n",
      " |      \n",
      " |      Args:\n",
      " |        inputs: Zero or more input tensors, packaged as described in the `Layer`\n",
      " |            class docstring.\n",
      " |      \n",
      " |      Returns:\n",
      " |        Zero or more output tensors, packaged as described in the `Layer` class\n",
      " |        docstring.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from trax.layers.base.Layer:\n",
      " |  \n",
      " |  __call__(self, x, weights=None, state=None, rng=None)\n",
      " |      Makes layers callable; for use in tests or interactive settings.\n",
      " |      \n",
      " |      This convenience method helps library users play with, test, or otherwise\n",
      " |      probe the behavior of layers outside of a full training environment. It\n",
      " |      presents the layer as callable function from inputs to outputs, with the\n",
      " |      option of manually specifying weights and non-parameter state per individual\n",
      " |      call. For convenience, weights and non-parameter state are cached per layer\n",
      " |      instance, starting from default values of `EMPTY_WEIGHTS` and `EMPTY_STATE`,\n",
      " |      and acquiring non-empty values either by initialization or from values\n",
      " |      explicitly provided via the weights and state keyword arguments.\n",
      " |      \n",
      " |      Args:\n",
      " |        x: Zero or more input tensors, packaged as described in the `Layer` class\n",
      " |            docstring.\n",
      " |        weights: Weights or `None`; if `None`, use self's cached weights value.\n",
      " |        state: State or `None`; if `None`, use self's cached state value.\n",
      " |        rng: Single-use random number generator (JAX PRNG key), or `None`;\n",
      " |            if `None`, use a default computed from an integer 0 seed.\n",
      " |      \n",
      " |      Returns:\n",
      " |        Zero or more output tensors, packaged as described in the `Layer` class\n",
      " |        docstring.\n",
      " |  \n",
      " |  __repr__(self)\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  backward(self, inputs, output, grad, weights, state, new_state, rng)\n",
      " |      Custom backward pass to propagate gradients in a custom way.\n",
      " |      \n",
      " |      Args:\n",
      " |        inputs: Input tensors; can be a (possibly nested) tuple.\n",
      " |        output: The result of running this layer on inputs.\n",
      " |        grad: Gradient signal computed based on subsequent layers; its structure\n",
      " |            and shape must match output.\n",
      " |        weights: This layer's weights.\n",
      " |        state: This layer's state prior to the current forward pass.\n",
      " |        new_state: This layer's state after the current forward pass.\n",
      " |        rng: Single-use random number generator (JAX PRNG key).\n",
      " |      \n",
      " |      Returns:\n",
      " |        The custom gradient signal for the input. Note that we need to return\n",
      " |        a gradient for each argument of forward, so it will usually be a tuple\n",
      " |        of signals: the gradient for inputs and weights.\n",
      " |  \n",
      " |  init(self, input_signature, rng=None, use_cache=False)\n",
      " |      Initializes weights/state of this layer and its sublayers recursively.\n",
      " |      \n",
      " |      Initialization creates layer weights and state, for layers that use them.\n",
      " |      It derives the necessary array shapes and data types from the layer's input\n",
      " |      signature, which is itself just shape and data type information.\n",
      " |      \n",
      " |      For layers without weights or state, this method safely does nothing.\n",
      " |      \n",
      " |      This method is designed to create weights/state only once for each layer\n",
      " |      instance, even if the same layer instance occurs in multiple places in the\n",
      " |      network. This enables weight sharing to be implemented as layer sharing.\n",
      " |      \n",
      " |      Args:\n",
      " |        input_signature: `ShapeDtype` instance (if this layer takes one input)\n",
      " |            or list/tuple of `ShapeDtype` instances.\n",
      " |        rng: Single-use random number generator (JAX PRNG key), or `None`;\n",
      " |            if `None`, use a default computed from an integer 0 seed.\n",
      " |        use_cache: If `True`, and if this layer instance has already been\n",
      " |            initialized elsewhere in the network, then return special marker\n",
      " |            values -- tuple `(GET_WEIGHTS_FROM_CACHE, GET_STATE_FROM_CACHE)`.\n",
      " |            Else return this layer's newly initialized weights and state.\n",
      " |      \n",
      " |      Returns:\n",
      " |        A `(weights, state)` tuple.\n",
      " |  \n",
      " |  init_from_file(self, file_name, weights_only=False, input_signature=None)\n",
      " |      Initializes this layer and its sublayers from a pickled checkpoint.\n",
      " |      \n",
      " |      In the common case (`weights_only=False`), the file must be a gziped pickled\n",
      " |      dictionary containing items with keys `'flat_weights', `'flat_state'` and\n",
      " |      `'input_signature'`, which are used to initialize this layer.\n",
      " |      If `input_signature` is specified, it's used instead of the one in the file.\n",
      " |      If `weights_only` is `True`, the dictionary does not need to have the\n",
      " |      `'flat_state'` item and the state it not restored either.\n",
      " |      \n",
      " |      Args:\n",
      " |        file_name: Name/path of the pickeled weights/state file.\n",
      " |        weights_only: If `True`, initialize only the layer's weights. Else\n",
      " |            initialize both weights and state.\n",
      " |        input_signature: Input signature to be used instead of the one from file.\n",
      " |  \n",
      " |  init_weights_and_state(self, input_signature)\n",
      " |      Initializes weights and state for inputs with the given signature.\n",
      " |      \n",
      " |      Authors of new layer subclasses should override this method if their layer\n",
      " |      uses trainable weights or non-trainable state. To initialize trainable\n",
      " |      weights, set `self.weights` and to initialize non-trainable state,\n",
      " |      set `self.state` to the intended value.\n",
      " |      \n",
      " |      Args:\n",
      " |        input_signature: A `ShapeDtype` instance (if this layer takes one input)\n",
      " |            or a list/tuple of `ShapeDtype` instances; signatures of inputs.\n",
      " |  \n",
      " |  output_signature(self, input_signature)\n",
      " |      Returns output signature this layer would give for `input_signature`.\n",
      " |  \n",
      " |  pure_fn(self, x, weights, state, rng, use_cache=False)\n",
      " |      Applies this layer as a pure function with no optional args.\n",
      " |      \n",
      " |      This method exposes the layer's computation as a pure function. This is\n",
      " |      especially useful for JIT compilation. Do not override, use `forward`\n",
      " |      instead.\n",
      " |      \n",
      " |      Args:\n",
      " |        x: Zero or more input tensors, packaged as described in the `Layer` class\n",
      " |            docstring.\n",
      " |        weights: A tuple or list of trainable weights, with one element for this\n",
      " |            layer if this layer has no sublayers, or one for each sublayer if\n",
      " |            this layer has sublayers. If a layer (or sublayer) has no trainable\n",
      " |            weights, the corresponding weights element is an empty tuple.\n",
      " |        state: Layer-specific non-parameter state that can update between batches.\n",
      " |        rng: Single-use random number generator (JAX PRNG key).\n",
      " |        use_cache: if `True`, cache weights and state in the layer object; used\n",
      " |          to implement layer sharing in combinators.\n",
      " |      \n",
      " |      Returns:\n",
      " |        A tuple of `(tensors, state)`. The tensors match the number (`n_out`)\n",
      " |        promised by this layer, and are packaged as described in the `Layer`\n",
      " |        class docstring.\n",
      " |  \n",
      " |  weights_and_state_signature(self, input_signature)\n",
      " |      Return a pair containing the signatures of weights and state.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from trax.layers.base.Layer:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  has_backward\n",
      " |      Returns `True` if this layer provides its own custom backward pass code.\n",
      " |      \n",
      " |      A layer subclass that provides custom backward pass code (for custom\n",
      " |      gradients) must override this method to return `True`.\n",
      " |  \n",
      " |  n_in\n",
      " |      Returns how many tensors this layer expects as input.\n",
      " |  \n",
      " |  n_out\n",
      " |      Returns how many tensors this layer promises as output.\n",
      " |  \n",
      " |  name\n",
      " |      Returns the name of this layer.\n",
      " |  \n",
      " |  rng\n",
      " |      Returns a single-use random number generator without advancing it.\n",
      " |  \n",
      " |  state\n",
      " |      Returns a tuple containing this layer's state; may be empty.\n",
      " |  \n",
      " |  sublayers\n",
      " |      Returns a tuple containing this layer's sublayers; may be empty.\n",
      " |  \n",
      " |  weights\n",
      " |      Returns this layer's weights.\n",
      " |      \n",
      " |      Depending on the layer, the weights can be in the form of:\n",
      " |      \n",
      " |        - an empty tuple\n",
      " |        - a tensor (ndarray)\n",
      " |        - a nested structure of tuples and tensors\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(tl.Concatenate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Normal shape: (4,) Data Type: <class 'tuple'>\nShapes Trax: ShapeDtype{shape:(4,), dtype:float64} Data Type: <class 'trax.shapes.ShapeDtype'>\n-- Properties --\nname : LayerNorm\nexpected inputs : 1\npromised outputs : 1\nweights : [1. 1. 1. 1.]\nbiases : [0. 0. 0. 0.] \n\n-- Inputs --\nx : [0. 1. 2. 3.]\n-- Outputs --\ny : [-1.3416404  -0.44721344  0.44721344  1.3416404 ]\n"
     ]
    }
   ],
   "source": [
    "# Layer initialization\n",
    "norm = tl.LayerNorm()\n",
    "# You first must know what the input data will look like\n",
    "x = np.array([0, 1, 2, 3], dtype=\"float\")\n",
    "\n",
    "# Use the input data signature to get shape and type for initializing weights and biases\n",
    "norm.init(shapes.signature(x)) # We need to convert the input datatype from usual tuple to trax ShapeDtype\n",
    "\n",
    "print(\"Normal shape:\",x.shape, \"Data Type:\",type(x.shape))\n",
    "print(\"Shapes Trax:\",shapes.signature(x),\"Data Type:\",type(shapes.signature(x)))\n",
    "\n",
    "# Inspect properties\n",
    "print(\"-- Properties --\")\n",
    "print(\"name :\", norm.name)\n",
    "print(\"expected inputs :\", norm.n_in)\n",
    "print(\"promised outputs :\", norm.n_out)\n",
    "# Weights and biases\n",
    "print(\"weights :\", norm.weights[0])\n",
    "print(\"biases :\", norm.weights[1], \"\\n\")\n",
    "\n",
    "# Inputs\n",
    "print(\"-- Inputs --\")\n",
    "print(\"x :\", x)\n",
    "\n",
    "# Outputs\n",
    "y = norm(x)\n",
    "print(\"-- Outputs --\")\n",
    "print(\"y :\", y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Help on function Fn in module trax.layers.base:\n\nFn(name, f, n_out=1)\n    Returns a layer with no weights that applies the function `f`.\n    \n    `f` can take and return any number of arguments, and takes only positional\n    arguments -- no default or keyword arguments. It often uses JAX-numpy (`jnp`).\n    The following, for example, would create a layer that takes two inputs and\n    returns two outputs -- element-wise sums and maxima:\n    \n        `Fn('SumAndMax', lambda x0, x1: (x0 + x1, jnp.maximum(x0, x1)), n_out=2)`\n    \n    The layer's number of inputs (`n_in`) is automatically set to number of\n    positional arguments in `f`, but you must explicitly set the number of\n    outputs (`n_out`) whenever it's not the default value 1.\n    \n    Args:\n      name: Class-like name for the resulting layer; for use in debugging.\n      f: Pure function from input tensors to output tensors, where each input\n          tensor is a separate positional arg, e.g., `f(x0, x1) --> x0 + x1`.\n          Output tensors must be packaged as specified in the `Layer` class\n          docstring.\n      n_out: Number of outputs promised by the layer; default value 1.\n    \n    Returns:\n      Layer executing the function `f`.\n\n"
     ]
    }
   ],
   "source": [
    "help(tl.Fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "-- Properties --\nname : TimesTwo\nexpected inputs : 1\npromised outputs : 1 \n\n-- Inputs --\nx : [1 2 3] \n\n-- Outputs --\ny : [2 4 6]\n"
     ]
    }
   ],
   "source": [
    "# Define a custom layer\n",
    "# In this example you will create a layer to calculate the input times 2\n",
    "\n",
    "def TimesTwo():\n",
    "    layer_name = \"TimesTwo\" #don't forget to give your custom layer a name to identify\n",
    "\n",
    "    # Custom function for the custom layer\n",
    "    def func(x):\n",
    "        return x * 2\n",
    "\n",
    "    return tl.Fn(layer_name, func)\n",
    "\n",
    "# Test it\n",
    "times_two = TimesTwo()\n",
    "\n",
    "# Inspect properties\n",
    "print(\"-- Properties --\")\n",
    "print(\"name :\", times_two.name)\n",
    "print(\"expected inputs :\", times_two.n_in)\n",
    "print(\"promised outputs :\", times_two.n_out, \"\\n\")\n",
    "\n",
    "# Inputs\n",
    "x = np.array([1, 2, 3])\n",
    "print(\"-- Inputs --\")\n",
    "print(\"x :\", x, \"\\n\")\n",
    "\n",
    "# Outputs\n",
    "y = times_two(x)\n",
    "print(\"-- Outputs --\")\n",
    "print(\"y :\", y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Help on class Parallel in module trax.layers.combinators:\n\nclass Parallel(trax.layers.base.Layer)\n |  Parallel(*sublayers, name=None)\n |  \n |  Combinator that applies a list of layers in parallel to its inputs.\n |  \n |  Layers in the list apply to successive spans of inputs, where the spans are\n |  determined how many inputs each layer takes. The resulting output is the\n |  (flattened) concatenation of the respective layer outputs.\n |  \n |  For example, suppose one has three layers:\n |  \n |    - F: 1 input, 1 output\n |    - G: 3 inputs, 1 output\n |    - H: 2 inputs, 2 outputs (h1, h2)\n |  \n |  Then Parallel(F, G, H) will take 6 inputs and give 4 outputs:\n |  \n |    - inputs: a, b, c, d, e, f\n |    - outputs: F(a), G(b, c, d), h1, h2\n |  \n |  As an important special case, a None argument to Parallel acts as if it takes\n |  one argument, which it leaves unchanged. (It acts as a one-arg no-op.) For\n |  example:\n |  \n |    Parallel(None, F)\n |  \n |  creates a layer that passes its first input unchanged and applies F to the\n |  following input(s).\n |  \n |  Method resolution order:\n |      Parallel\n |      trax.layers.base.Layer\n |      builtins.object\n |  \n |  Methods defined here:\n |  \n |  __init__(self, *sublayers, name=None)\n |      The constructor.\n |      \n |      Args:\n |        *sublayers: A list of sublayers.\n |        name: Descriptive name for this layer.\n |      \n |      Returns:\n |        A new layer in which each of the given sublayers applies to its\n |        corresponding span of elements in the dataflow stack.\n |  \n |  forward(self, inputs)\n |      Computes this layer's output as part of a forward pass through the model.\n |      \n |      Authors of new layer subclasses should override this method to define the\n |      forward computation that their layer performs. Use `self.weights` to access\n |      trainable weights of this layer. If you need to use local non-trainable\n |      state or randomness, use `self.rng` for the random seed (no need to set it)\n |      and use `self.state` for non-trainable state (and set it to the new value).\n |      \n |      Args:\n |        inputs: Zero or more input tensors, packaged as described in the `Layer`\n |            class docstring.\n |      \n |      Returns:\n |        Zero or more output tensors, packaged as described in the `Layer` class\n |        docstring.\n |  \n |  init_weights_and_state(self, input_signature)\n |      Initializes weights and state for inputs with the given signature.\n |      \n |      Authors of new layer subclasses should override this method if their layer\n |      uses trainable weights or non-trainable state. To initialize trainable\n |      weights, set `self.weights` and to initialize non-trainable state,\n |      set `self.state` to the intended value.\n |      \n |      Args:\n |        input_signature: A `ShapeDtype` instance (if this layer takes one input)\n |            or a list/tuple of `ShapeDtype` instances; signatures of inputs.\n |  \n |  ----------------------------------------------------------------------\n |  Data descriptors defined here:\n |  \n |  state\n |      Returns a tuple containing this layer's state; may be empty.\n |  \n |  weights\n |      Returns this layer's weights.\n |      \n |      Depending on the layer, the weights can be in the form of:\n |      \n |        - an empty tuple\n |        - a tensor (ndarray)\n |        - a nested structure of tuples and tensors\n |  \n |  ----------------------------------------------------------------------\n |  Methods inherited from trax.layers.base.Layer:\n |  \n |  __call__(self, x, weights=None, state=None, rng=None)\n |      Makes layers callable; for use in tests or interactive settings.\n |      \n |      This convenience method helps library users play with, test, or otherwise\n |      probe the behavior of layers outside of a full training environment. It\n |      presents the layer as callable function from inputs to outputs, with the\n |      option of manually specifying weights and non-parameter state per individual\n |      call. For convenience, weights and non-parameter state are cached per layer\n |      instance, starting from default values of `EMPTY_WEIGHTS` and `EMPTY_STATE`,\n |      and acquiring non-empty values either by initialization or from values\n |      explicitly provided via the weights and state keyword arguments.\n |      \n |      Args:\n |        x: Zero or more input tensors, packaged as described in the `Layer` class\n |            docstring.\n |        weights: Weights or `None`; if `None`, use self's cached weights value.\n |        state: State or `None`; if `None`, use self's cached state value.\n |        rng: Single-use random number generator (JAX PRNG key), or `None`;\n |            if `None`, use a default computed from an integer 0 seed.\n |      \n |      Returns:\n |        Zero or more output tensors, packaged as described in the `Layer` class\n |        docstring.\n |  \n |  __repr__(self)\n |      Return repr(self).\n |  \n |  backward(self, inputs, output, grad, weights, state, new_state, rng)\n |      Custom backward pass to propagate gradients in a custom way.\n |      \n |      Args:\n |        inputs: Input tensors; can be a (possibly nested) tuple.\n |        output: The result of running this layer on inputs.\n |        grad: Gradient signal computed based on subsequent layers; its structure\n |            and shape must match output.\n |        weights: This layer's weights.\n |        state: This layer's state prior to the current forward pass.\n |        new_state: This layer's state after the current forward pass.\n |        rng: Single-use random number generator (JAX PRNG key).\n |      \n |      Returns:\n |        The custom gradient signal for the input. Note that we need to return\n |        a gradient for each argument of forward, so it will usually be a tuple\n |        of signals: the gradient for inputs and weights.\n |  \n |  init(self, input_signature, rng=None, use_cache=False)\n |      Initializes weights/state of this layer and its sublayers recursively.\n |      \n |      Initialization creates layer weights and state, for layers that use them.\n |      It derives the necessary array shapes and data types from the layer's input\n |      signature, which is itself just shape and data type information.\n |      \n |      For layers without weights or state, this method safely does nothing.\n |      \n |      This method is designed to create weights/state only once for each layer\n |      instance, even if the same layer instance occurs in multiple places in the\n |      network. This enables weight sharing to be implemented as layer sharing.\n |      \n |      Args:\n |        input_signature: `ShapeDtype` instance (if this layer takes one input)\n |            or list/tuple of `ShapeDtype` instances.\n |        rng: Single-use random number generator (JAX PRNG key), or `None`;\n |            if `None`, use a default computed from an integer 0 seed.\n |        use_cache: If `True`, and if this layer instance has already been\n |            initialized elsewhere in the network, then return special marker\n |            values -- tuple `(GET_WEIGHTS_FROM_CACHE, GET_STATE_FROM_CACHE)`.\n |            Else return this layer's newly initialized weights and state.\n |      \n |      Returns:\n |        A `(weights, state)` tuple.\n |  \n |  init_from_file(self, file_name, weights_only=False, input_signature=None)\n |      Initializes this layer and its sublayers from a pickled checkpoint.\n |      \n |      In the common case (`weights_only=False`), the file must be a gziped pickled\n |      dictionary containing items with keys `'flat_weights', `'flat_state'` and\n |      `'input_signature'`, which are used to initialize this layer.\n |      If `input_signature` is specified, it's used instead of the one in the file.\n |      If `weights_only` is `True`, the dictionary does not need to have the\n |      `'flat_state'` item and the state it not restored either.\n |      \n |      Args:\n |        file_name: Name/path of the pickeled weights/state file.\n |        weights_only: If `True`, initialize only the layer's weights. Else\n |            initialize both weights and state.\n |        input_signature: Input signature to be used instead of the one from file.\n |  \n |  output_signature(self, input_signature)\n |      Returns output signature this layer would give for `input_signature`.\n |  \n |  pure_fn(self, x, weights, state, rng, use_cache=False)\n |      Applies this layer as a pure function with no optional args.\n |      \n |      This method exposes the layer's computation as a pure function. This is\n |      especially useful for JIT compilation. Do not override, use `forward`\n |      instead.\n |      \n |      Args:\n |        x: Zero or more input tensors, packaged as described in the `Layer` class\n |            docstring.\n |        weights: A tuple or list of trainable weights, with one element for this\n |            layer if this layer has no sublayers, or one for each sublayer if\n |            this layer has sublayers. If a layer (or sublayer) has no trainable\n |            weights, the corresponding weights element is an empty tuple.\n |        state: Layer-specific non-parameter state that can update between batches.\n |        rng: Single-use random number generator (JAX PRNG key).\n |        use_cache: if `True`, cache weights and state in the layer object; used\n |          to implement layer sharing in combinators.\n |      \n |      Returns:\n |        A tuple of `(tensors, state)`. The tensors match the number (`n_out`)\n |        promised by this layer, and are packaged as described in the `Layer`\n |        class docstring.\n |  \n |  weights_and_state_signature(self, input_signature)\n |      Return a pair containing the signatures of weights and state.\n |  \n |  ----------------------------------------------------------------------\n |  Data descriptors inherited from trax.layers.base.Layer:\n |  \n |  __dict__\n |      dictionary for instance variables (if defined)\n |  \n |  __weakref__\n |      list of weak references to the object (if defined)\n |  \n |  has_backward\n |      Returns `True` if this layer provides its own custom backward pass code.\n |      \n |      A layer subclass that provides custom backward pass code (for custom\n |      gradients) must override this method to return `True`.\n |  \n |  n_in\n |      Returns how many tensors this layer expects as input.\n |  \n |  n_out\n |      Returns how many tensors this layer promises as output.\n |  \n |  name\n |      Returns the name of this layer.\n |  \n |  rng\n |      Returns a single-use random number generator without advancing it.\n |  \n |  sublayers\n |      Returns a tuple containing this layer's sublayers; may be empty.\n\n"
     ]
    }
   ],
   "source": [
    "# combinators are used to combine layers into more complex layers\n",
    "# Serial Combinator is most common and easiest to use.\n",
    "# help(tl.Serial)\n",
    "help(tl.Parallel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "-- Serial Model --\nSerial[\n  LayerNorm\n  Relu\n  TimesTwo\n  Dense_2\n  LogSoftmax\n] \n\n-- Properties --\nname : Serial\nsublayers : [LayerNorm, Relu, TimesTwo, Dense_2, LogSoftmax]\nexpected inputs : 1\npromised outputs : 1\nweights & biases: [(DeviceArray([1, 1, 1, 1, 1], dtype=int32), DeviceArray([0, 0, 0, 0, 0], dtype=int32)), (), (), (DeviceArray([[ 0.47118202, -0.7620581 ],\n             [ 0.3756661 , -0.4218606 ],\n             [ 0.15808171, -0.07303074],\n             [-0.2424559 , -0.12089023],\n             [-0.2705172 , -0.8956146 ]], dtype=float32), DeviceArray([8.7000068e-08, 1.6511547e-06], dtype=float32)), ()] \n\n-- Inputs --\nx : [-2 -1  0  1  2] \n\n-- Outputs --\ny : [-0.1845535 -1.7806741]\n"
     ]
    }
   ],
   "source": [
    "# Serial combinator\n",
    "serial = tl.Serial(\n",
    "    tl.LayerNorm(),         # normalize input\n",
    "    tl.Relu(),              # convert negative values to zero\n",
    "    times_two,              # the custom layer you created above, multiplies the input recieved from above by 2\n",
    "    \n",
    "    ### START CODE HERE\n",
    "    tl.Dense(n_units=2),  # try adding more layers. eg uncomment these lines\n",
    "    # tl.Dense(n_units=1),  # Binary classification, maybe? uncomment at your own peril\n",
    "    tl.LogSoftmax()       # Yes, LogSoftmax is also a layer\n",
    "    ### END CODE HERE\n",
    ")\n",
    "\n",
    "# Initialization\n",
    "x = np.array([-2, -1, 0, 1, 2]) #input\n",
    "serial.init(shapes.signature(x)) #initialising serial instance\n",
    "\n",
    "print(\"-- Serial Model --\")\n",
    "print(serial,\"\\n\")\n",
    "print(\"-- Properties --\")\n",
    "print(\"name :\", serial.name)\n",
    "print(\"sublayers :\", serial.sublayers)\n",
    "print(\"expected inputs :\", serial.n_in)\n",
    "print(\"promised outputs :\", serial.n_out)\n",
    "print(\"weights & biases:\", serial.weights, \"\\n\")\n",
    "\n",
    "# Inputs\n",
    "print(\"-- Inputs --\")\n",
    "print(\"x :\", x, \"\\n\")\n",
    "\n",
    "# Outputs\n",
    "y = serial(x)\n",
    "print(\"-- Outputs --\")\n",
    "print(\"y :\", y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "good old numpy :  <class 'numpy.ndarray'> \n\njax trax numpy :  <class 'jax.interpreters.xla._DeviceArray'>\n"
     ]
    }
   ],
   "source": [
    "# Jax\n",
    "# Numpy vs fastmath.numpy have different data types\n",
    "# Regular ol' numpy\n",
    "x_numpy = np.array([1, 2, 3])\n",
    "print(\"good old numpy : \", type(x_numpy), \"\\n\")\n",
    "\n",
    "# Fastmath and jax numpy\n",
    "x_jax = fastmath.numpy.array([1, 2, 3])\n",
    "print(\"jax trax numpy : \", type(x_jax))"
   ]
  },
  {
   "source": [
    "# Classes and subclasses"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "None\nNone\n"
     ]
    }
   ],
   "source": [
    "class My_Class:\n",
    "    x = None\n",
    "\n",
    "instance_a = My_Class()\n",
    "instance_b = My_Class()\n",
    "print(str(instance_a.x))\n",
    "print(str(instance_b.x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "5\n"
     ]
    }
   ],
   "source": [
    "instance_a.x = 5\n",
    "print(instance_a.x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "metadata": {},
     "execution_count": 32
    }
   ],
   "source": [
    "class My_Class: \n",
    "    def __init__(self, y): # The __init__ method takes as input the instance to be initialized and a variable y\n",
    "        self.x = y         # Sets parameter x to be equal to y\n",
    "\n",
    "instance_c = My_Class(10)\n",
    "instance_c.x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "15\n"
     ]
    }
   ],
   "source": [
    "# __call__ method\n",
    "class My_Class:\n",
    "    def __init__(self, y):\n",
    "        self.x = y\n",
    "    \n",
    "    def __call__(self, z):\n",
    "        self.x +=  z\n",
    "        print(self.x)\n",
    "\n",
    "instance_d = My_Class(5)\n",
    "instance_d(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Addition of 10 and 15 is 25\n\u001b[92mAll tests passed!\n"
     ]
    }
   ],
   "source": [
    "class My_Class: \n",
    "    def __init__(self, y, z): #Initialization of x_1 and x_2 with arguments y and z\n",
    "        ### START CODE HERE (2 lines) ### \n",
    "        self.x_1 = y\n",
    "        self.x_2 = z\n",
    "        ### END CODE HERE ###\n",
    "    def __call__(self):       #When called, adds the values of parameters x_1 and x_2, prints and returns the result \n",
    "        ### START CODE HERE (1 line) ### \n",
    "        result = self.x_1 + self.x_2 \n",
    "        ### END CODE HERE ### \n",
    "        print(\"Addition of {} and {} is {}\".format(self.x_1,self.x_2,result))\n",
    "        return result\n",
    "\n",
    "instance_e = My_Class(10,15)\n",
    "def test_class_definition():\n",
    "    \n",
    "    assert instance_e.x_1 == 10, \"Check the value assigned to x_1\"\n",
    "    assert instance_e.x_2 == 15, \"Check the value assigned to x_2\"\n",
    "    assert instance_e() == 25, \"Check the __call__ method\"\n",
    "    \n",
    "    print(\"\\033[92mAll tests passed!\")\n",
    "    \n",
    "test_class_definition()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "26"
      ]
     },
     "metadata": {},
     "execution_count": 35
    }
   ],
   "source": [
    "class My_Class: \n",
    "    def __init__(self, y, z): #Initialization of x_1 and x_2 with arguments y and z\n",
    "        self.x_1 = y\n",
    "        self.x_2 = z\n",
    "    def __call__(self):       #Performs an operation with x_1 and x_2, and returns the result\n",
    "        a = self.x_1 - 2*self.x_2 \n",
    "        return a\n",
    "    def my_method(self, w):   #Multiplies x_1 and x_2, adds argument w and returns the result\n",
    "        result = self.x_1*self.x_2 + w\n",
    "        return result\n",
    "\n",
    "instance_f = My_Class(1, 10)\n",
    "instance_f.my_method(16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "26\n26\n"
     ]
    }
   ],
   "source": [
    "# hidden-cell\n",
    "class My_Class: \n",
    "    def __init__(self, y, z):      #Initialization of x_1 and x_2 with arguments y and z\n",
    "        self.x_1 = y\n",
    "        self.x_2 = z\n",
    "    def __call__(self):            #Performs an operation with x_1 and x_2, and returns the result\n",
    "        a = self.x_1 - 2*self.x_2 \n",
    "        return a\n",
    "    def my_method(self, w):        #Multiplies x_1 and x_2, adds argument w and returns the result\n",
    "        b = self.x_1*self.x_2 + w\n",
    "        return b\n",
    "    def new_method(self, v):       #Calls My_method with argument v\n",
    "        result = self.my_method(v)\n",
    "        return result\n",
    "\n",
    "instance_g = My_Class(1,10)\n",
    "print(instance_g.my_method(16))\n",
    "print(instance_g.new_method(16))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part 2: Subclasses and Inheritance\n",
    "class sub_c(My_Class):           #Subclass sub_c from My_class\n",
    "    def additional_method(self): #Prints the value of parameter x_1\n",
    "        print(self.x_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Parameter x_1 of instance_sub_a: 1\nParameter x_2 of instance_sub_a: 10\nOutput of my_method of instance_sub_a: 26\n"
     ]
    }
   ],
   "source": [
    "instance_sub_a = sub_c(1,10)\n",
    "print('Parameter x_1 of instance_sub_a: ' + str(instance_sub_a.x_1))\n",
    "print('Parameter x_2 of instance_sub_a: ' + str(instance_sub_a.x_2))\n",
    "print(\"Output of my_method of instance_sub_a:\",instance_sub_a.my_method(16))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Output of overridden my_method of test: 30\n"
     ]
    }
   ],
   "source": [
    "class sub_c(My_Class):           #Subclass sub_c from My_class\n",
    "    def my_method(self):         #Multiplies x_1 and x_2 and returns the result\n",
    "        ### START CODE HERE (1 line) ###\n",
    "        b = self.x_1*self.x_2 \n",
    "        ### END CODE HERE ###\n",
    "        return b\n",
    "\n",
    "test = sub_c(3,10)\n",
    "assert test.my_method() == 30, \"The method my_method should return the product between x_1 and x_2\"\n",
    "\n",
    "print(\"Output of overridden my_method of test:\",test.my_method())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "My_method for an instance of sub_c returns: 10\nMy_method for an instance of My_Class returns: 20\n"
     ]
    }
   ],
   "source": [
    "y,z= 1,10\n",
    "instance_sub_a = sub_c(y,z)\n",
    "instance_a = My_Class(y,z)\n",
    "print('My_method for an instance of sub_c returns: ' + str(instance_sub_a.my_method()))\n",
    "print('My_method for an instance of My_Class returns: ' + str(instance_a.my_method(10)))"
   ]
  },
  {
   "source": [
    "# Data generators"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[1, 2, 3, 4, 1, 2, 3, 4, 1, 2]\n"
     ]
    }
   ],
   "source": [
    "# Generator is like an iterator, it returns the net item. Useful to handle loading and transforming data for different applications\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "# Traversing a list of indexes to create a circular list\n",
    "a = [1, 2, 3, 4]\n",
    "b = [0] * 10\n",
    "\n",
    "a_size = len(a)\n",
    "b_size = len(b)\n",
    "lines_index = [*range(a_size)]\n",
    "index = 0\n",
    "for i in range(b_size): # b is longer than a, forcing a wrap\n",
    "    if index >= a_size:\n",
    "        index = 0\n",
    "\n",
    "    b[i] = a[lines_index[index]]\n",
    "    index += 1\n",
    "\n",
    "print(b)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Original order of index: [0, 1, 2, 3]\nShuffled order of index: [1, 2, 0, 3]\nNew value order for first batch: [2, 3, 1, 4]\n\nShuffled Indexes for Batch No.2 :[0, 2, 3, 1]\nValues for Batch No.2 :[1, 3, 4, 2]\n\nShuffled Indexes for Batch No.3 :[1, 0, 2, 3]\nValues for Batch No.3 :[2, 1, 3, 4]\n\nFinal value of b: [2, 3, 1, 4, 1, 3, 4, 2, 2, 1]\n"
     ]
    }
   ],
   "source": [
    "# Shuffling the data order\n",
    "# Example of traversing a list of indexes to create a circular list\n",
    "a = [1, 2, 3, 4]\n",
    "b = []\n",
    "\n",
    "a_size = len(a)\n",
    "b_size = 10\n",
    "lines_index = [*range(a_size)]\n",
    "print(\"Original order of index:\",lines_index)\n",
    "\n",
    "# if we shuffle the index_list we can change the order of our circular list\n",
    "# without modifying the order or our original data\n",
    "random.shuffle(lines_index) # Shuffle the order\n",
    "print(\"Shuffled order of index:\",lines_index)\n",
    "\n",
    "print(\"New value order for first batch:\",[a[index] for index in lines_index])\n",
    "batch_counter = 1\n",
    "index = 0                # similar to index in data_generator below\n",
    "for i in range(b_size):  # `b` is longer than `a` forcing a wrap\n",
    "    # We wrap by resetting index to 0\n",
    "    if index >= a_size:\n",
    "        index = 0\n",
    "        batch_counter += 1\n",
    "        random.shuffle(lines_index) # Re-shuffle the order\n",
    "        print(\"\\nShuffled Indexes for Batch No.{} :{}\".format(batch_counter,lines_index))\n",
    "        print(\"Values for Batch No.{} :{}\".format(batch_counter,[a[index] for index in lines_index]))\n",
    "    \n",
    "    b.append(a[lines_index[index]])     #  `indexes_list[index]` point to a index of a. Store the result in b\n",
    "    index += 1\n",
    "print()    \n",
    "print(\"Final value of b:\",b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement a data generator function that takes in batch_size, x, y shuffle\n",
    "def data_generator(batch_size, data_x, data_y, shuffle=True):\n",
    "    data_lng = len(data_x)\n",
    "    index_list = [*range(data_lng)]\n",
    "\n",
    "    if shuffle:\n",
    "        random.shuffle(index_list)\n",
    "    \n",
    "    index = 0\n",
    "    while True:\n",
    "        X = [None] * batch_size\n",
    "        Y = [None] * batch_size\n",
    "\n",
    "        for i in range(batch_size):\n",
    "            #Wrap the index each time that we reach the end of the list\n",
    "            if index >= data_lng:\n",
    "                index=0\n",
    "                if shuffle:\n",
    "                    random.shuffle(index_list)\n",
    "\n",
    "            X[i] = data_x[index_list[index]]\n",
    "            Y[i] = data_y[index_list[index]]\n",
    "\n",
    "            index += 1\n",
    "        yield((X,Y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\u001b[92mAll tests passed!\n"
     ]
    }
   ],
   "source": [
    "def test_data_generator():\n",
    "    x = [1, 2, 3, 4]\n",
    "    y = [xi ** 2 for xi in x]\n",
    "    \n",
    "    generator = data_generator(3, x, y, shuffle=False)\n",
    "\n",
    "    assert np.allclose(next(generator), ([1, 2, 3], [1, 4, 9])),  \"First batch does not match\"\n",
    "    assert np.allclose(next(generator), ([4, 1, 2], [16, 1, 4])), \"Second batch does not match\"\n",
    "    assert np.allclose(next(generator), ([3, 4, 1], [9, 16, 1])), \"Third batch does not match\"\n",
    "    assert np.allclose(next(generator), ([2, 3, 4], [4, 9, 16])), \"Fourth batch does not match\"\n",
    "\n",
    "    print(\"\\033[92mAll tests passed!\")\n",
    "\n",
    "test_data_generator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "def data_generator(batch_size, data_x, data_y, shuffle=True):\n\n    data_lng = len(data_x) # len(data_x) must be equal to len(data_y)\n    index_list = [*range(data_lng)] # Create a list with the ordered indexes of sample data\n    \n    # If shuffle is set to true, we traverse the list in a random way\n    if shuffle:\n        rnd.shuffle(index_list) # Inplace shuffle of the list\n    \n    index = 0 # Start with the first element\n    while True:\n        X = [0] * batch_size # We can create a list with batch_size elements. \n        Y = [0] * batch_size # We can create a list with batch_size elements. \n        \n        for i in range(batch_size):\n            \n            # Wrap the index each time that we reach the end of the list\n            if index >= data_lng:\n                index = 0\n                # Shuffle the index_list if shuffle is true\n                if shuffle:\n                    rnd.shuffle(index_list) # re-shuffle the order\n            \n            X[i] = data_x[index_list[index]] \n            Y[i] = data_y[index_list[index]] \n            \n            index += 1\n        \n        yield((X, Y))\n"
     ]
    }
   ],
   "source": [
    "import base64\n",
    "\n",
    "solution = \"ZGVmIGRhdGFfZ2VuZXJhdG9yKGJhdGNoX3NpemUsIGRhdGFfeCwgZGF0YV95LCBzaHVmZmxlPVRydWUpOgoKICAgIGRhdGFfbG5nID0gbGVuKGRhdGFfeCkgIyBsZW4oZGF0YV94KSBtdXN0IGJlIGVxdWFsIHRvIGxlbihkYXRhX3kpCiAgICBpbmRleF9saXN0ID0gWypyYW5nZShkYXRhX2xuZyldICMgQ3JlYXRlIGEgbGlzdCB3aXRoIHRoZSBvcmRlcmVkIGluZGV4ZXMgb2Ygc2FtcGxlIGRhdGEKICAgIAogICAgIyBJZiBzaHVmZmxlIGlzIHNldCB0byB0cnVlLCB3ZSB0cmF2ZXJzZSB0aGUgbGlzdCBpbiBhIHJhbmRvbSB3YXkKICAgIGlmIHNodWZmbGU6CiAgICAgICAgcm5kLnNodWZmbGUoaW5kZXhfbGlzdCkgIyBJbnBsYWNlIHNodWZmbGUgb2YgdGhlIGxpc3QKICAgIAogICAgaW5kZXggPSAwICMgU3RhcnQgd2l0aCB0aGUgZmlyc3QgZWxlbWVudAogICAgd2hpbGUgVHJ1ZToKICAgICAgICBYID0gWzBdICogYmF0Y2hfc2l6ZSAjIFdlIGNhbiBjcmVhdGUgYSBsaXN0IHdpdGggYmF0Y2hfc2l6ZSBlbGVtZW50cy4gCiAgICAgICAgWSA9IFswXSAqIGJhdGNoX3NpemUgIyBXZSBjYW4gY3JlYXRlIGEgbGlzdCB3aXRoIGJhdGNoX3NpemUgZWxlbWVudHMuIAogICAgICAgIAogICAgICAgIGZvciBpIGluIHJhbmdlKGJhdGNoX3NpemUpOgogICAgICAgICAgICAKICAgICAgICAgICAgIyBXcmFwIHRoZSBpbmRleCBlYWNoIHRpbWUgdGhhdCB3ZSByZWFjaCB0aGUgZW5kIG9mIHRoZSBsaXN0CiAgICAgICAgICAgIGlmIGluZGV4ID49IGRhdGFfbG5nOgogICAgICAgICAgICAgICAgaW5kZXggPSAwCiAgICAgICAgICAgICAgICAjIFNodWZmbGUgdGhlIGluZGV4X2xpc3QgaWYgc2h1ZmZsZSBpcyB0cnVlCiAgICAgICAgICAgICAgICBpZiBzaHVmZmxlOgogICAgICAgICAgICAgICAgICAgIHJuZC5zaHVmZmxlKGluZGV4X2xpc3QpICMgcmUtc2h1ZmZsZSB0aGUgb3JkZXIKICAgICAgICAgICAgCiAgICAgICAgICAgIFhbaV0gPSBkYXRhX3hbaW5kZXhfbGlzdFtpbmRleF1dIAogICAgICAgICAgICBZW2ldID0gZGF0YV95W2luZGV4X2xpc3RbaW5kZXhdXSAKICAgICAgICAgICAgCiAgICAgICAgICAgIGluZGV4ICs9IDEKICAgICAgICAKICAgICAgICB5aWVsZCgoWCwgWSkp\"\n",
    "\n",
    "# Print the solution to the given assignment\n",
    "print(base64.b64decode(solution).decode(\"utf-8\"))"
   ]
  }
 ]
}