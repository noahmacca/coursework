{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/noahcowan-\n",
      "[nltk_data]     maccallum/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "source": [
    "import nltk\n",
    "import re\n",
    "import ssl\n",
    "\n",
    "try:\n",
    "    _create_unverified_https_context = ssl._create_unverified_context\n",
    "except AttributeError:\n",
    "    pass\n",
    "else:\n",
    "    ssl._create_default_https_context = _create_unverified_https_context\n",
    "\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "learning makes me happy. i am happy because i am learning! \n"
     ]
    }
   ],
   "source": [
    "corpus = \"Learning% makes 'me' happy. I am happy be-cause I am learning! :)\"\n",
    "\n",
    "# lowercase\n",
    "corpus = corpus.lower()\n",
    "\n",
    "# remove special characters\n",
    "corpus = re.sub(r\"[^a-zA-Z0-9.?! ]+\", \"\", corpus)\n",
    "# Note that this gets rid of emoticon, which is important for sentiment but not this task.\n",
    "\n",
    "print(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['Sat', 'May', '', '9', '07:33:35', 'CEST', '2020']\n['07', '33', '35']\n"
     ]
    }
   ],
   "source": [
    "# Split newlines into list of sentences\n",
    "input_date=\"Sat May  9 07:33:35 CEST 2020\"\n",
    "\n",
    "print(input_date.split(' '))\n",
    "print(input_date.split(' ')[4].split(':'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['i', 'am', 'happy', 'because', 'I', 'am', 'learning', '.']\n"
     ]
    }
   ],
   "source": [
    "# Split each sentence into a list of words, using nltk\n",
    "sentence = 'i am happy because I am learning.'\n",
    "tokenized_sentence = nltk.word_tokenize(sentence)\n",
    "print(tokenized_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[('i', 1), ('am', 2), ('happy', 5), ('because', 7), ('I', 1), ('am', 2), ('learning', 8), ('.', 1)]\n"
     ]
    }
   ],
   "source": [
    "# Find the length of each word\n",
    "print([(word, len(word)) for word in tokenized_sentence])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['i', 'am', 'happy']\n['am', 'happy', 'because']\n['happy', 'because', 'I']\n['because', 'I', 'am']\n['I', 'am', 'learning']\n['am', 'learning', '.']\n['learning', '.']\n['.']\n"
     ]
    }
   ],
   "source": [
    "# Sentence to n-gram. Window scans the list of words frmo beginning to end.corpus\n",
    "\n",
    "def sentence_to_trigram(tokenized_sentence):\n",
    "    # Prints all trigrams in the given tokenized sentence.\n",
    "    for i in range(len(tokenized_sentence)):\n",
    "        trigram = tokenized_sentence[i:i + 3]\n",
    "        print(trigram)\n",
    "\n",
    "sentence_to_trigram(tokenized_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['i', 'am', 'happy']\n"
     ]
    }
   ],
   "source": [
    "# Prefix of an n-gram. This is needed to calculate the probability of an n-gram\n",
    "# Get an n-1-gram prefix from n-gram\n",
    "\n",
    "fourgram = ['i', 'am', 'happy', 'because']\n",
    "trigram = fourgram[0:-1]\n",
    "print(trigram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['<s>', '<s>', 'i', 'am', 'happy', 'because', 'I', 'am', 'learning', '.', '</s>']\n"
     ]
    }
   ],
   "source": [
    "# Start and end of sentence word. Prepend n-1 to beginning and one to the end\n",
    "n = 3\n",
    "print(['<s>'] * (n-1) + tokenized_sentence + ['</s>'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "2\nmiss\n('i', 'am', 'happy', 'because')\n"
     ]
    }
   ],
   "source": [
    "# Building the language model\n",
    "# count matrix - frequencies of n-grams and n-gram prefixes in the training dataset. Count of (n-1)-gram prefix followed by all possible last words in the vocabulary.\n",
    "\n",
    "n_gram_counts = {\n",
    "    ('i', 'am', 'happy'): 2,\n",
    "    ('am', 'happy', 'because'): 1}\n",
    "\n",
    "# get count for an n-gram tuple\n",
    "print(n_gram_counts[('i', 'am', 'happy')])\n",
    "\n",
    "# check if n-gram is present in the dict\n",
    "if ('i', 'am', 'learning') in n_gram_counts:\n",
    "    print('hit')\n",
    "else:\n",
    "    print('miss')\n",
    "\n",
    "# concat tuple for prefix and tuple with the last word to create the n-gram\n",
    "prefix = ('i', 'am', 'happy')\n",
    "word = 'because'\n",
    "n_gram = prefix + (word,)\n",
    "print(n_gram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "defaultdict(<class 'dict'>, {(('i', 'am'), 'happy'): 1, (('am', 'happy'), 'because'): 1, (('happy', 'because'), 'i'): 1, (('because', 'i'), 'am'): 1, (('i', 'am'), 'learning'): 1, (('am', 'learning'), '.'): 1})\n                  happy  because    i   am  learning    .\n(i, am)             1.0      0.0  0.0  0.0       1.0  0.0\n(am, happy)         0.0      1.0  0.0  0.0       0.0  0.0\n(happy, because)    0.0      0.0  1.0  0.0       0.0  0.0\n(because, i)        0.0      0.0  0.0  1.0       0.0  0.0\n(am, learning)      0.0      0.0  0.0  0.0       0.0  1.0\n"
     ]
    }
   ],
   "source": [
    "# sliding window to populate trigram count matrix\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "\n",
    "def single_pass_trigram_count_matrix(corpus):\n",
    "    # corpus is pre-processed and tokenized\n",
    "    # Returns list of all bigrams, vocabulary of all found words, dataframe with bigram prefixes as rows and vocab words as columns\n",
    "    bigrams = []\n",
    "    vocabulary = []\n",
    "    count_matrix_dict = defaultdict(dict)\n",
    "\n",
    "    # go through the corpus once with a sliding window\n",
    "    for i in range(len(corpus) - 3 + 1):\n",
    "        # The sliding window starts at position i and contains 3 words\n",
    "        trigram = tuple(corpus[i:i+3])\n",
    "        bigram = trigram[0:-1]\n",
    "        if not bigram in bigrams:\n",
    "            bigrams.append(bigram)\n",
    "\n",
    "        last_word = trigram[-1]\n",
    "        if not last_word in vocabulary:\n",
    "            vocabulary.append(last_word)\n",
    "\n",
    "        if (bigram, last_word) not in count_matrix_dict:\n",
    "            count_matrix_dict[bigram, last_word] = 0\n",
    "        \n",
    "        count_matrix_dict[bigram,last_word] += 1\n",
    "    \n",
    "    print(count_matrix_dict)\n",
    "    # convert the count_matrix to np.array to fill in the blanks\n",
    "    count_matrix = np.zeros((len(bigrams), len(vocabulary)))\n",
    "    for trigram_key, trigram_count in count_matrix_dict.items():\n",
    "        count_matrix[bigrams.index(trigram_key[0]), \\\n",
    "                        vocabulary.index(trigram_key[1])] = \\\n",
    "                        trigram_count\n",
    "    # np.array to pandas ataframe conversion\n",
    "    count_matrix = pd.DataFrame(count_matrix, index=bigrams, columns=vocabulary)\n",
    "    return bigrams, vocabulary, count_matrix\n",
    "\n",
    "corpus = ['i', 'am', 'happy', 'because', 'i', 'am', 'learning', '.']\n",
    "\n",
    "bigrams, vocabulary, count_matrix = single_pass_trigram_count_matrix(corpus)\n",
    "\n",
    "print(count_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "                  happy  because    i   am  learning    .\n(i, am)             0.5      0.0  0.0  0.0       0.5  0.0\n(am, happy)         0.0      1.0  0.0  0.0       0.0  0.0\n(happy, because)    0.0      0.0  1.0  0.0       0.0  0.0\n(because, i)        0.0      0.0  0.0  1.0       0.0  0.0\n(am, learning)      0.0      0.0  0.0  0.0       0.0  1.0\n"
     ]
    }
   ],
   "source": [
    "# Probability matrix\n",
    "row_sums = count_matrix.sum(axis=1)\n",
    "prob_matrix = count_matrix.div(row_sums, axis=0)\n",
    "\n",
    "print(prob_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "('i', 'am')\nhappy\n0.5\n"
     ]
    }
   ],
   "source": [
    "# Find the probability of a trigram in the probability matrix\n",
    "trigram = ('i', 'am', 'happy')\n",
    "bigram = trigram[:-1]\n",
    "print(bigram)\n",
    "\n",
    "word = trigram[-1]\n",
    "print(word)\n",
    "\n",
    "trigram_probability = prob_matrix[word][bigram]\n",
    "print(trigram_probability)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "words in vocabulary starting with prefix: ha\nhappy\nhave\n"
     ]
    }
   ],
   "source": [
    "# list all words in vocabulary starting with a given prefix\n",
    "vocabulary = ['i', 'am', 'happy', 'because', 'learning', '.', 'have', 'you', 'seen','it', '?']\n",
    "starts_with = 'ha'\n",
    "\n",
    "print(f'words in vocabulary starting with prefix: {starts_with}')\n",
    "for word in vocabulary:\n",
    "    if word.startswith(starts_with):\n",
    "        print(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "([28, 76, 5, 0, 62, 29, 54, 95, 88, 58, 4, 22, 92, 14, 50, 77, 47, 33, 75, 68, 56, 74, 43, 80, 83, 84, 73, 93, 66, 87, 9, 91, 64, 79, 20, 51, 17, 27, 12, 31, 67, 81, 7, 34, 45, 72, 38, 30, 16, 60, 40, 86, 48, 21, 70, 59, 6, 19, 2, 99, 37, 36, 52, 61, 97, 44, 26, 57, 89, 55, 53, 85, 3, 39, 10, 71, 23, 32, 25, 8], [78, 65, 63, 11, 49, 98, 1, 46, 15, 41], [90, 96, 82, 42, 35, 13, 69, 24, 94, 18])\n"
     ]
    }
   ],
   "source": [
    "# Language model evaluation\n",
    "# Can randomly sample corpus to define test and validation set\n",
    "import random\n",
    "\n",
    "def train_validation_test_split(data, train_percent, validation_percent):\n",
    "    random.seed(87)\n",
    "\n",
    "    random.shuffle(data)\n",
    "\n",
    "    train_size = int(len(data) * train_percent / 100)\n",
    "    train_data = data[0:train_size]\n",
    "\n",
    "    validation_size = int(len(data) * validation_percent / 100)\n",
    "    validation_data = data[train_size:train_size + validation_size]\n",
    "\n",
    "    test_data = data[train_size + validation_size:]\n",
    "\n",
    "    return train_data, validation_data, test_data\n",
    "\n",
    "data = [x for x in range(0,100)]\n",
    "print(train_validation_test_split(data, 80, 10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "1e-250 100\n316.22776601683796\n"
     ]
    }
   ],
   "source": [
    "# Perplexity\n",
    "p = 10 ** (-250)\n",
    "M = 100\n",
    "print(p, M)\n",
    "perplexity = p ** (-1/M)\n",
    "print(perplexity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[('happy', 5), ('because', 3), ('learning', 3)]\n['happy', 'because', 'learning']\n"
     ]
    }
   ],
   "source": [
    "# Out of vocabulary words (OOV)\n",
    "\n",
    "# Build the vocabulary form M most frequent words\n",
    "from collections import Counter\n",
    "\n",
    "M = 3\n",
    "\n",
    "# Can use counter to build this\n",
    "word_counts = {'happy': 5, 'because': 3, 'i': 2, 'am': 2, 'learning': 3, '.': 1}\n",
    "vocabulary = Counter(word_counts).most_common(M)\n",
    "print(vocabulary)\n",
    "\n",
    "vocabulary = [w[0] for w in vocabulary]\n",
    "print(vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['am', 'i', 'learning']\n['<UNK>', '<UNK>', 'learning']\n"
     ]
    }
   ],
   "source": [
    "# replace OOV words with <unk>\n",
    "sentence = ['am', 'i', 'learning']\n",
    "output_sentence = []\n",
    "print(sentence)\n",
    "\n",
    "for w in sentence:\n",
    "    # test if word w is in vocabulary\n",
    "    if w in vocabulary:\n",
    "        output_sentence.append(w)\n",
    "    else:\n",
    "        output_sentence.append('<UNK>')\n",
    "\n",
    "print(output_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "because\nlearning\n"
     ]
    }
   ],
   "source": [
    "f = 3\n",
    "word_counts = {'happy': 5, 'because': 3, 'i': 2, 'am': 2, 'learning':3, '.': 1}\n",
    "\n",
    "for word, freq in word_counts.items():\n",
    "    if freq == f:\n",
    "        print(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "1.2599210498948732\n1.0\n"
     ]
    }
   ],
   "source": [
    "# If there are many <unk>, may get low perplexity, as shown\n",
    "training_set = ['i', 'am', 'happy', 'because','i', 'am', 'learning', '.']\n",
    "training_set_unk = ['i', 'am', '<UNK>', '<UNK>','i', 'am', '<UNK>', '<UNK>']\n",
    "\n",
    "test_set = ['i', 'am', 'learning']\n",
    "test_set_unk = ['i', 'am', '<UNK>']\n",
    "\n",
    "M = len(test_set)\n",
    "probability = 1\n",
    "probability_unk = 1\n",
    "\n",
    "# pre-calculated probabilities\n",
    "bigram_probabilities = {('i', 'am'): 1.0, ('am', 'happy'): 0.5, ('happy', 'because'): 1.0, ('because', 'i'): 1.0, ('am', 'learning'): 0.5, ('learning', '.'): 1.0}\n",
    "bigram_probabilities_unk = {('i', 'am'): 1.0, ('am', '<UNK>'): 1.0, ('<UNK>', '<UNK>'): 0.5, ('<UNK>', 'i'): 0.25}\n",
    "\n",
    "# Go through test set and calculate its bigram probability\n",
    "for i in range(len(test_set) - 2 + 1):\n",
    "    bigram = tuple(test_set[i:i+2])\n",
    "    probability = probability * bigram_probabilities[bigram]\n",
    "\n",
    "    bigram_unk = tuple(test_set_unk[i:i+2])\n",
    "    probability_unk = probability_unk * bigram_probabilities_unk[bigram_unk]\n",
    "\n",
    "perplexity = probability ** (-1/M)\n",
    "perplexity_unk = probability_unk ** (-1/M)\n",
    "\n",
    "print(perplexity)\n",
    "print(perplexity_unk)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0.2\n0.2\n"
     ]
    }
   ],
   "source": [
    "# Smoothing. Bad that unknown phrase gets same probability as one that is in the test set\n",
    "def add_k_smoothing_probability(k, vocabulary_size, n_gram_count, n_gram_prefix_count):\n",
    "    numerator = n_gram_count + k\n",
    "    denominator = n_gram_prefix_count + k * vocabulary_size\n",
    "    return numerator/denominator\n",
    "\n",
    "trigram_probabilities = {('i', 'am', 'happy'): 2}\n",
    "bigram_probabilities = {('am', 'happy'): 10}\n",
    "vocabulary_size = 5\n",
    "k = 1\n",
    "\n",
    "probability_known_trigram = add_k_smoothing_probability(k, vocabulary_size, trigram_probabilities[('i', 'am', 'happy')], \n",
    "                           bigram_probabilities[( 'am', 'happy')])\n",
    "\n",
    "probability_unknown_trigram = add_k_smoothing_probability(k, vocabulary_size, 0, 0)\n",
    "\n",
    "print(probability_known_trigram)\n",
    "print(probability_unknown_trigram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "besides the trigram ('are', 'you', 'happy') we also use bigram ('you', 'happy') and unigram (happy)\n\nprobability for trigram ('are', 'you', 'happy') not found\nprobability for bigram ('you', 'happy') not found\nprobability for unigram happy found\n\nprobability for trigram ('are', 'you', 'happy') estimated as 0.06400000000000002\n"
     ]
    }
   ],
   "source": [
    "# Back-off. Use lower order n-grams in case information about high order n-grams is missing\n",
    "# pre-calculated probabilities of all types of n-grams\n",
    "trigram_probabilities = {('i', 'am', 'happy'): 0}\n",
    "bigram_probabilities = {( 'am', 'happy'): 0.3}\n",
    "unigram_probabilities = {'happy': 0.4}\n",
    "\n",
    "# this is the input trigram we need to estimate\n",
    "trigram = ('are', 'you', 'happy')\n",
    "\n",
    "# find the last bigram and unigram of the input\n",
    "bigram = trigram[1: 3]\n",
    "unigram = trigram[2]\n",
    "print(f\"besides the trigram {trigram} we also use bigram {bigram} and unigram ({unigram})\\n\")\n",
    "\n",
    "# 0.4 is used as an example, experimentally found for web-scale corpuses when using the \"stupid\" back-off\n",
    "lambda_factor = 0.4\n",
    "probability_hat_trigram = 0\n",
    "\n",
    "# search for first non-zero probability starting with trigram\n",
    "# to generalize this for any order of n-gram hierarchy, \n",
    "# you could loop through the probability dictionaries instead of if/else cascade\n",
    "if trigram not in trigram_probabilities or trigram_probabilities[trigram] == 0:\n",
    "    print(f\"probability for trigram {trigram} not found\")\n",
    "    \n",
    "    if bigram not in bigram_probabilities or bigram_probabilities[bigram] == 0:\n",
    "        print(f\"probability for bigram {bigram} not found\")\n",
    "        \n",
    "        if unigram in unigram_probabilities:\n",
    "            print(f\"probability for unigram {unigram} found\\n\")\n",
    "            probability_hat_trigram = lambda_factor * lambda_factor * unigram_probabilities[unigram]\n",
    "        else:\n",
    "            probability_hat_trigram = 0\n",
    "    else:\n",
    "        probability_hat_trigram = lambda_factor * bigram_probabilities[bigram]\n",
    "else:\n",
    "    probability_hat_trigram = trigram_probabilities[trigram]\n",
    "\n",
    "print(f\"probability for trigram {trigram} estimated as {probability_hat_trigram}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "besides the trigram ('i', 'am', 'happy') we also use bigram ('am', 'happy') and unigram (happy)\n\nestimated probability of the input trigram ('i', 'am', 'happy') is 0.12\n"
     ]
    }
   ],
   "source": [
    "# Interpolation. use weighted probabilities of  n-grams of all orders every time, not just when high order information is missing\n",
    "\n",
    "# pre-calculated probabilities of all types of n-grams\n",
    "trigram_probabilities = {('i', 'am', 'happy'): 0.15}\n",
    "bigram_probabilities = {( 'am', 'happy'): 0.3}\n",
    "unigram_probabilities = {'happy': 0.4}\n",
    "\n",
    "# the weights come from optimization on a validation set\n",
    "lambda_1 = 0.8\n",
    "lambda_2 = 0.15\n",
    "lambda_3 = 0.05\n",
    "\n",
    "# this is the input trigram we need to estimate\n",
    "trigram = ('i', 'am', 'happy')\n",
    "\n",
    "# find the last bigram and unigram of the input\n",
    "bigram = trigram[1: 3]\n",
    "unigram = trigram[2]\n",
    "print(f\"besides the trigram {trigram} we also use bigram {bigram} and unigram ({unigram})\\n\")\n",
    "\n",
    "# in the production code, you would need to check if the probability n-gram dictionary contains the n-gram\n",
    "probability_hat_trigram = lambda_1 * trigram_probabilities[trigram] \n",
    "+ lambda_2 * bigram_probabilities[bigram]\n",
    "+ lambda_3 * unigram_probabilities[unigram]\n",
    "\n",
    "print(f\"estimated probability of the input trigram {trigram} is {probability_hat_trigram}\")\n"
   ]
  }
 ]
}